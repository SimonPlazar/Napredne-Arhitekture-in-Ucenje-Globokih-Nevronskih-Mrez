{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-21T14:41:36.464445Z",
     "start_time": "2025-11-21T14:41:01.214905Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "# Import checkpoint utilities\n",
    "from Model import save_checkpoint_generic, load_checkpoint_generic"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T14:41:36.523987Z",
     "start_time": "2025-11-21T14:41:36.504474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # DCGAN Training Setup\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#\n",
    "# batch_size = 512\n",
    "# image_size = 64\n",
    "# nc = 3\n",
    "# noise_size = 100\n",
    "# num_epochs = 500\n",
    "# lr = 0.0002\n",
    "# beta1 = 0.5\n",
    "#\n",
    "# # Transforms\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize(image_size),\n",
    "#     transforms.CenterCrop(image_size),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.5] * nc, [0.5] * nc),\n",
    "# ])\n",
    "#\n",
    "# from Model import DCGAN_Dataset\n",
    "# data_root = \"./Dataset/tpdne_faces\"\n",
    "# out_dir = \"./output_dcgan\"\n",
    "# if not os.path.exists(data_root):\n",
    "#     raise FileNotFoundError(\"Dataset not found\")\n",
    "# os.makedirs(out_dir, exist_ok=True)\n",
    "#\n",
    "# dataset = DCGAN_Dataset(data_root, transform=transform)\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     dataset,\n",
    "#     # batch_size=batch_size,\n",
    "#     batch_size=len(dataset),\n",
    "#     shuffle=True,\n",
    "#     num_workers=0,\n",
    "#     drop_last=False,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "#\n",
    "# # seed\n",
    "# manualSeed = 999\n",
    "# random.seed(manualSeed)\n",
    "# torch.manual_seed(manualSeed)\n",
    "#\n",
    "# # Initialize models\n",
    "# from Model import DCGAN_Generator, DCGAN_Discriminator\n",
    "# generator = DCGAN_Generator(z_dim=noise_size, img_channels=nc).to(device)\n",
    "# discriminator = DCGAN_Discriminator(img_channels=nc).to(device)\n",
    "#\n",
    "# criterionG = nn.BCELoss()\n",
    "# criterionD = nn.BCELoss()\n",
    "#\n",
    "# optimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999), foreach=False)\n",
    "# optimizerD = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999), foreach=False)\n",
    "#\n",
    "# # Training config to save with checkpoints\n",
    "# training_config = {\n",
    "#     'batch_size': batch_size,\n",
    "#     'image_size': image_size,\n",
    "#     'nc': nc,\n",
    "#     'noise_size': noise_size,\n",
    "#     'num_epochs': num_epochs,\n",
    "#     'lr': lr,\n",
    "#     'beta1': beta1,\n",
    "#     'manualSeed': manualSeed,\n",
    "# }\n"
   ],
   "id": "9a2dfaaa42471005",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T14:41:36.572899Z",
     "start_time": "2025-11-21T14:41:36.555048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # TRAIN LOOP DCGAN\n",
    "# torch.cuda.empty_cache()\n",
    "#\n",
    "# # Initialize variables first\n",
    "# start_epoch = 0\n",
    "# G_losses = []\n",
    "# D_losses = []\n",
    "# loaded_noise = None\n",
    "#\n",
    "# real_label = 1.\n",
    "# fake_label = 0.\n",
    "# img_list = []\n",
    "#\n",
    "# # Load checkpoint (if exists)\n",
    "# checkpoint = load_checkpoint_generic(out_dir, device)\n",
    "# # checkpoint = load_checkpoint_generic_compat(out_dir, device)\n",
    "# if checkpoint:\n",
    "#     generator.load_state_dict(checkpoint['generator'])\n",
    "#     discriminator.load_state_dict(checkpoint['discriminator'])\n",
    "#     optimizerG.load_state_dict(checkpoint['optimizer_G'])\n",
    "#     optimizerD.load_state_dict(checkpoint['optimizer_D'])\n",
    "#     start_epoch = checkpoint['epoch']\n",
    "#     G_losses = checkpoint.get('G_losses', [])\n",
    "#     D_losses = checkpoint.get('D_losses', [])\n",
    "#     loaded_noise = checkpoint.get('fixed_noise')\n",
    "#     config = checkpoint.get('config', {})\n",
    "#\n",
    "# # Initialize or use loaded fixed noise\n",
    "# if loaded_noise is not None:\n",
    "#     fixed_noise = loaded_noise.to(device)\n",
    "#     print(\"Using loaded fixed_noise from checkpoint\")\n",
    "# else:\n",
    "#     fixed_noise = torch.randn(64, noise_size, 1, 1, device=device)\n",
    "#     print(\"Created new fixed_noise for visualization\")\n",
    "#\n",
    "# print(\"Starting Training Loop on device:\", device)\n",
    "# print(f\"Training from epoch {start_epoch + 1} to {num_epochs}\")\n",
    "# iters = len(G_losses)\n",
    "# start_time = time.time()\n",
    "#\n",
    "# for epoch in range(start_epoch + 1, num_epochs + 1):\n",
    "#     epoch_start = time.time()\n",
    "#\n",
    "#     # ‚úÖ Set models to training mode\n",
    "#     generator.train()\n",
    "#     discriminator.train()\n",
    "#\n",
    "#     for i, data in enumerate(dataloader, 0):\n",
    "#\n",
    "#         ############################\n",
    "#         # (1) Update Discriminator\n",
    "#         ############################\n",
    "#         discriminator.zero_grad()\n",
    "#\n",
    "#         real_images = data.to(device)\n",
    "#         b_size = real_images.size(0)\n",
    "#         label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "#\n",
    "#         output = discriminator(real_images).view(-1)\n",
    "#         errD_real = criterionD(output, label)\n",
    "#         errD_real.backward()\n",
    "#         D_x = output.mean().item()\n",
    "#\n",
    "#         noise = torch.randn(b_size, noise_size, 1, 1, device=device)\n",
    "#         fake = generator(noise)\n",
    "#         label.fill_(fake_label)\n",
    "#\n",
    "#         output = discriminator(fake.detach()).view(-1)\n",
    "#         errD_fake = criterionD(output, label)\n",
    "#         errD_fake.backward()\n",
    "#         D_G_z1 = output.mean().item()\n",
    "#\n",
    "#         errD = errD_real + errD_fake\n",
    "#         optimizerD.step()\n",
    "#\n",
    "#         ############################\n",
    "#         # (2) Update Generator\n",
    "#         ############################\n",
    "#         generator.zero_grad()\n",
    "#         label.fill_(real_label)\n",
    "#\n",
    "#         output = discriminator(fake).view(-1)\n",
    "#         errG = criterionG(output, label)\n",
    "#         errG.backward()\n",
    "#         D_G_z2 = output.mean().item()\n",
    "#\n",
    "#         optimizerG.step()\n",
    "#\n",
    "#         G_losses.append(errG.item())\n",
    "#         D_losses.append(errD.item())\n",
    "#\n",
    "#         if i % 50 == 0:\n",
    "#             print(f\"Epoch [{epoch}/{num_epochs}] Batch [{i}/{len(dataloader)}] \"\n",
    "#                   f\"Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f} \"\n",
    "#                   f\"D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f}/{D_G_z2:.4f}\")\n",
    "#\n",
    "#         iters += 1\n",
    "#\n",
    "#     generator.eval()\n",
    "#\n",
    "#     # End of epoch: save sample grid\n",
    "#     with torch.no_grad():\n",
    "#         fake_fixed = generator(fixed_noise).detach().cpu()\n",
    "#\n",
    "#     grid = make_grid(fake_fixed, nrow=8, normalize=True, scale_each=True)\n",
    "#     epoch_img_path = os.path.join(out_dir, f\"epoch_{epoch:04d}.png\")\n",
    "#     save_image(grid, epoch_img_path)\n",
    "#     img_list.append(epoch_img_path)\n",
    "#\n",
    "#     # ‚úÖ Clear memory only after epoch (not every batch)\n",
    "#     del fake_fixed, grid\n",
    "#     torch.cuda.empty_cache()\n",
    "#\n",
    "#     epoch_time = time.time() - epoch_start\n",
    "#     print(f\"End epoch {epoch}/{num_epochs}  time: {epoch_time:.1f}s  saved: {epoch_img_path}\")\n",
    "#\n",
    "#     if epoch % 10 == 0 or epoch == num_epochs:\n",
    "#         # Save\n",
    "#         save_checkpoint_generic(out_dir, epoch, {\n",
    "#             'generator': generator.state_dict(),\n",
    "#             'discriminator': discriminator.state_dict(),\n",
    "#             'optimizer_G': optimizerG.state_dict(),\n",
    "#             'optimizer_D': optimizerD.state_dict(),\n",
    "#             'G_losses': G_losses,\n",
    "#             'D_losses': D_losses,\n",
    "#             'fixed_noise': fixed_noise,\n",
    "#             'config': training_config,\n",
    "#         })\n",
    "#\n",
    "#\n",
    "# total_time = time.time() - start_time\n",
    "# print(f\"Training finished in {total_time / 60:.2f} minutes.\")"
   ],
   "id": "6361ebc2e7afd6df",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T14:41:36.614278Z",
     "start_time": "2025-11-21T14:41:36.596986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # CycleGAN Training Setup\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#\n",
    "# # dataset_root = \"./Dataset/apple2orange\"\n",
    "# dataset_root = \"./Dataset/Summer2Winter_Yosemite\"\n",
    "# data_root_A = os.path.join(dataset_root, \"trainA\")\n",
    "# data_root_B = os.path.join(dataset_root, \"trainB\")\n",
    "# test_root_A = os.path.join(dataset_root, \"testA\")\n",
    "# test_root_B = os.path.join(dataset_root, \"testB\")\n",
    "# # data_root_A = \"./Dataset/apple2orange/trainA\"\n",
    "# # data_root_B = \"./Dataset/apple2orange/trainB\"\n",
    "# # test_root_A = \"./Dataset/apple2orange/testA\"\n",
    "# # test_root_B = \"./Dataset/apple2orange/testB\"\n",
    "#\n",
    "# # out_dir = \"output_cyclegan_a2o\"\n",
    "# out_dir = \"output_cyclegan_s2w\"\n",
    "# image_size = 256\n",
    "# batch_size = 4\n",
    "# num_epochs = 50\n",
    "# lr = 0.0002\n",
    "# beta1 = 0.5\n",
    "# lambda_cycle = 10.0\n",
    "# lambda_identity = 5.0\n",
    "#\n",
    "# # Transforms\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((image_size, image_size)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "# ])\n",
    "#\n",
    "# from Model import CycleGAN_Dataset\n",
    "#\n",
    "# os.makedirs(out_dir, exist_ok=True)\n",
    "#\n",
    "# # Training dataset\n",
    "# dataset = CycleGAN_Dataset(data_root_A, data_root_B, transform=transform)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "#\n",
    "# # Test dataset (fixed samples)\n",
    "# test_dataset = CycleGAN_Dataset(test_root_A, test_root_B, transform=transform)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "#\n",
    "# # Seed\n",
    "# manualSeed = 999\n",
    "# random.seed(manualSeed)\n",
    "# torch.manual_seed(manualSeed)\n",
    "#\n",
    "# # Initialize models\n",
    "# from Model import CycleGAN_Generator, CycleGAN_Discriminator\n",
    "#\n",
    "# G_A2B = CycleGAN_Generator().to(device)\n",
    "# G_B2A = CycleGAN_Generator().to(device)\n",
    "# D_A = CycleGAN_Discriminator().to(device)\n",
    "# D_B = CycleGAN_Discriminator().to(device)\n",
    "#\n",
    "# # Loss functions\n",
    "# criterion_GAN = nn.MSELoss()\n",
    "# criterion_cycle = nn.L1Loss()\n",
    "# criterion_identity = nn.L1Loss()\n",
    "#\n",
    "# # Optimizers\n",
    "# optimizer_G = optim.Adam(\n",
    "#     list(G_A2B.parameters()) + list(G_B2A.parameters()),\n",
    "#     lr=lr, betas=(beta1, 0.999)\n",
    "# )\n",
    "# optimizer_D_A = optim.Adam(D_A.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "# optimizer_D_B = optim.Adam(D_B.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "#\n",
    "# # Training config\n",
    "# training_config = {\n",
    "#     'batch_size': batch_size,\n",
    "#     'image_size': image_size,\n",
    "#     'num_epochs': num_epochs,\n",
    "#     'lr': lr,\n",
    "#     'beta1': beta1,\n",
    "#     'lambda_cycle': lambda_cycle,\n",
    "#     'lambda_identity': lambda_identity,\n",
    "#     'manualSeed': manualSeed,\n",
    "# }"
   ],
   "id": "6f793eef3e03f99e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T14:41:36.664906Z",
     "start_time": "2025-11-21T14:41:36.638379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Training loop CycleGAN\n",
    "# torch.cuda.empty_cache()\n",
    "#\n",
    "# start_epoch = 0\n",
    "# G_losses = []\n",
    "# D_A_losses = []\n",
    "# D_B_losses = []\n",
    "# test_A = None\n",
    "# test_B = None\n",
    "#\n",
    "# # Load checkpoint\n",
    "# checkpoint = load_checkpoint_generic(out_dir, device)\n",
    "# if checkpoint:\n",
    "#     G_A2B.load_state_dict(checkpoint['G_A2B'])\n",
    "#     G_B2A.load_state_dict(checkpoint['G_B2A'])\n",
    "#     D_A.load_state_dict(checkpoint['D_A'])\n",
    "#     D_B.load_state_dict(checkpoint['D_B'])\n",
    "#     optimizer_G.load_state_dict(checkpoint['optimizer_G'])\n",
    "#     optimizer_D_A.load_state_dict(checkpoint['optimizer_D_A'])\n",
    "#     optimizer_D_B.load_state_dict(checkpoint['optimizer_D_B'])\n",
    "#     start_epoch = checkpoint['epoch']\n",
    "#     G_losses = checkpoint.get('G_losses', [])\n",
    "#     D_A_losses = checkpoint.get('D_A_losses', [])\n",
    "#     D_B_losses = checkpoint.get('D_B_losses', [])\n",
    "#     test_A = checkpoint.get('test_A')\n",
    "#     test_B = checkpoint.get('test_B')\n",
    "#     config = checkpoint.get('config', {})\n",
    "#\n",
    "# # Get fixed test samples (or use loaded ones)\n",
    "# if test_A is None or test_B is None:\n",
    "#     test_A, test_B = next(iter(test_loader))\n",
    "#     test_A = test_A.to(device)\n",
    "#     test_B = test_B.to(device)\n",
    "#     print(\"Created new fixed test samples\")\n",
    "# else:\n",
    "#     test_A = test_A.to(device)\n",
    "#     test_B = test_B.to(device)\n",
    "#     print(\"Using loaded fixed test samples from checkpoint\")\n",
    "#\n",
    "# print(f\"Starting CycleGAN training on {device}\")\n",
    "# print(f\"Training from epoch {start_epoch + 1} to {num_epochs}\")\n",
    "# start_time = time.time()\n",
    "#\n",
    "# for epoch in range(start_epoch + 1, num_epochs + 1):\n",
    "#     epoch_start = time.time()\n",
    "#\n",
    "#     G_A2B.train()\n",
    "#     G_B2A.train()\n",
    "#     D_A.train()\n",
    "#     D_B.train()\n",
    "#\n",
    "#     for i, (real_A, real_B) in enumerate(dataloader):\n",
    "#         batch_start = time.time()\n",
    "#         real_A = real_A.to(device)\n",
    "#         real_B = real_B.to(device)\n",
    "#\n",
    "#         batch_size_curr = real_A.size(0)\n",
    "#         real_label = torch.ones(batch_size_curr, 1, 30, 30, device=device)\n",
    "#         fake_label = torch.zeros(batch_size_curr, 1, 30, 30, device=device)\n",
    "#\n",
    "#         # =====================================\n",
    "#         # Train Generators\n",
    "#         # =====================================\n",
    "#         optimizer_G.zero_grad()\n",
    "#\n",
    "#         # Identity loss\n",
    "#         identity_A = G_B2A(real_A)\n",
    "#         loss_identity_A = criterion_identity(identity_A, real_A)\n",
    "#\n",
    "#         identity_B = G_A2B(real_B)\n",
    "#         loss_identity_B = criterion_identity(identity_B, real_B)\n",
    "#\n",
    "#         # GAN loss\n",
    "#         fake_B = G_A2B(real_A)\n",
    "#         pred_fake_B = D_B(fake_B)\n",
    "#         loss_GAN_A2B = criterion_GAN(pred_fake_B, real_label)\n",
    "#\n",
    "#         fake_A = G_B2A(real_B)\n",
    "#         pred_fake_A = D_A(fake_A)\n",
    "#         loss_GAN_B2A = criterion_GAN(pred_fake_A, real_label)\n",
    "#\n",
    "#         # Cycle consistency loss\n",
    "#         recovered_A = G_B2A(fake_B)\n",
    "#         loss_cycle_A = criterion_cycle(recovered_A, real_A)\n",
    "#\n",
    "#         recovered_B = G_A2B(fake_A)\n",
    "#         loss_cycle_B = criterion_cycle(recovered_B, real_B)\n",
    "#\n",
    "#         # Total generator loss\n",
    "#         loss_G = (\n",
    "#                 loss_GAN_A2B + loss_GAN_B2A +\n",
    "#                 lambda_cycle * (loss_cycle_A + loss_cycle_B) +\n",
    "#                 lambda_identity * (loss_identity_A + loss_identity_B)\n",
    "#         )\n",
    "#\n",
    "#         loss_G.backward()\n",
    "#         optimizer_G.step()\n",
    "#\n",
    "#         # =====================================\n",
    "#         # Train Discriminator A\n",
    "#         # =====================================\n",
    "#         optimizer_D_A.zero_grad()\n",
    "#\n",
    "#         pred_real_A = D_A(real_A)\n",
    "#         loss_D_real_A = criterion_GAN(pred_real_A, real_label)\n",
    "#\n",
    "#         pred_fake_A = D_A(fake_A.detach())\n",
    "#         loss_D_fake_A = criterion_GAN(pred_fake_A, fake_label)\n",
    "#\n",
    "#         loss_D_A = loss_D_real_A + loss_D_fake_A\n",
    "#         loss_D_A.backward()\n",
    "#         optimizer_D_A.step()\n",
    "#\n",
    "#         # =====================================\n",
    "#         # Train Discriminator B\n",
    "#         # =====================================\n",
    "#         optimizer_D_B.zero_grad()\n",
    "#\n",
    "#         pred_real_B = D_B(real_B)\n",
    "#         loss_D_real_B = criterion_GAN(pred_real_B, real_label)\n",
    "#\n",
    "#         pred_fake_B = D_B(fake_B.detach())\n",
    "#         loss_D_fake_B = criterion_GAN(pred_fake_B, fake_label)\n",
    "#\n",
    "#         loss_D_B = loss_D_real_B + loss_D_fake_B\n",
    "#         loss_D_B.backward()\n",
    "#         optimizer_D_B.step()\n",
    "#\n",
    "#         # Track losses\n",
    "#         G_losses.append(loss_G.item())\n",
    "#         D_A_losses.append(loss_D_A.item())\n",
    "#         D_B_losses.append(loss_D_B.item())\n",
    "#\n",
    "#         batch_time = time.time() - batch_start\n",
    "#         print(f\"Epoch [{epoch}/{num_epochs}] Batch [{i}/{len(dataloader)}] Time: {batch_time:.2f}s\")\n",
    "#\n",
    "#         if i % 1 == 0:\n",
    "#             print(f\"Loss_G: {loss_G.item():.4f} \"\n",
    "#                   f\"Loss_D_A: {loss_D_A.item():.4f} \"\n",
    "#                   f\"Loss_D_B: {loss_D_B.item():.4f}\")\n",
    "#\n",
    "#     # Save sample images at end of epoch\n",
    "#     G_A2B.eval()\n",
    "#     G_B2A.eval()\n",
    "#\n",
    "#     with torch.no_grad():\n",
    "#         fake_B_sample = G_A2B(test_A)\n",
    "#         fake_A_sample = G_B2A(test_B)\n",
    "#\n",
    "#         comparison = torch.cat([test_A, fake_B_sample, test_B, fake_A_sample])\n",
    "#         grid = make_grid(comparison, nrow=8, normalize=True, scale_each=True)\n",
    "#         save_image(grid, os.path.join(out_dir, f\"epoch_{epoch:04d}.png\"))\n",
    "#\n",
    "#     epoch_time = time.time() - epoch_start\n",
    "#     print(f\"Epoch {epoch}/{num_epochs} completed in {epoch_time:.1f}s\")\n",
    "#\n",
    "#     # Save checkpoint\n",
    "#     if epoch % 1 == 0 or epoch == num_epochs:\n",
    "#         save_checkpoint_generic(out_dir, epoch, {\n",
    "#             'G_A2B': G_A2B.state_dict(),\n",
    "#             'G_B2A': G_B2A.state_dict(),\n",
    "#             'D_A': D_A.state_dict(),\n",
    "#             'D_B': D_B.state_dict(),\n",
    "#             'optimizer_G': optimizer_G.state_dict(),\n",
    "#             'optimizer_D_A': optimizer_D_A.state_dict(),\n",
    "#             'optimizer_D_B': optimizer_D_B.state_dict(),\n",
    "#             'G_losses': G_losses,\n",
    "#             'D_A_losses': D_A_losses,\n",
    "#             'D_B_losses': D_B_losses,\n",
    "#             'test_A': test_A.cpu(),\n",
    "#             'test_B': test_B.cpu(),\n",
    "#             'config': training_config,\n",
    "#         })\n",
    "#     torch.cuda.empty_cache()\n",
    "#\n",
    "# total_time = time.time() - start_time\n",
    "# print(f\"Training finished in {total_time / 60:.2f} minutes\")"
   ],
   "id": "4da8a15471ac83ea",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T15:39:47.824216Z",
     "start_time": "2025-11-21T15:27:22.082657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NST\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "output_dir = \"output_nst\"\n",
    "input_dir  = \"Dataset/NST\"\n",
    "# style_image_path   = os.path.join(input_dir, \"nst-picasso.jpg\")\n",
    "style_image_path   = os.path.join(input_dir, \"nst-kadinsky.jpg\")\n",
    "content_image_path = os.path.join(input_dir, \"nst-dancing.jpg\")\n",
    "# content_image_path = os.path.join(input_dir, \"nst-labrador.jpg\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ---------------- PREPROCESS / POSTPROCESS ----------------\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "])\n",
    "\n",
    "# When saving, undo normalization and clamp to [0,1]\n",
    "def denormalize(tensor):\n",
    "    # tensor shape: (1,3,H,W)\n",
    "    t = tensor.clone().detach().cpu()\n",
    "    for c, (m, s) in enumerate(zip(imagenet_mean, imagenet_std)):\n",
    "        t[:, c, :, :] = t[:, c, :, :] * s + m\n",
    "    t = torch.clamp(t, 0.0, 1.0)\n",
    "    return t\n",
    "\n",
    "def load_img(path, size=512):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    return preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "# ---------------- LOAD IMAGES ----------------\n",
    "content_img = load_img(content_image_path)\n",
    "style_img   = load_img(style_image_path)\n",
    "\n",
    "# ---------------- VGG19 SETUP ----------------\n",
    "vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features\n",
    "vgg = vgg.to(device).eval()\n",
    "\n",
    "# FIX: Important for LBFGS ‚Äî disable inplace ReLU!\n",
    "for i, layer in enumerate(vgg):\n",
    "    if isinstance(layer, nn.ReLU):\n",
    "        vgg[i] = nn.ReLU(inplace=False)\n",
    "\n",
    "for p in vgg.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "# mapping: actual layer indices in torchvision VGG19.features\n",
    "layer_mapping = {\n",
    "    0:  \"conv1_1\",\n",
    "    5:  \"conv2_1\",\n",
    "    10: \"conv3_1\",\n",
    "    19: \"conv4_1\",\n",
    "    21: \"conv4_2\",   # content layer\n",
    "    28: \"conv5_1\"\n",
    "}\n",
    "\n",
    "style_layers   = [\"conv1_1\", \"conv2_1\", \"conv3_1\", \"conv4_1\", \"conv5_1\"]\n",
    "content_layers = [\"conv4_2\"]\n",
    "\n",
    "# Wrap vgg into sequential 'model' (same as vgg but easier to enumerate)\n",
    "model = nn.Sequential(*list(vgg))\n",
    "\n",
    "# ----------------- GRAM -----------------\n",
    "def gram_matrix(t):\n",
    "    b, c, h, w = t.size()\n",
    "    f = t.view(b, c, h * w)\n",
    "    g = torch.bmm(f, f.transpose(1, 2))     # (b, c, c)\n",
    "    return g / (c * h * w)\n",
    "\n",
    "# ---------------- EXTRACT TARGETS ----------------\n",
    "style_targets = {}\n",
    "content_targets = {}\n",
    "\n",
    "# Forward pass through all layers and capture at indices\n",
    "x = style_img.clone()\n",
    "y = content_img.clone()\n",
    "\n",
    "for idx, layer in enumerate(model):\n",
    "    x = layer(x)\n",
    "    y = layer(y)\n",
    "    if idx in layer_mapping:\n",
    "        name = layer_mapping[idx]\n",
    "        if name in style_layers:\n",
    "            style_targets[name] = gram_matrix(x).detach()\n",
    "        if name in content_layers:\n",
    "            content_targets[name] = y.detach()\n",
    "\n",
    "print(\"Captured style target layers:\", list(style_targets.keys()))\n",
    "print(\"Captured content target layers:\", list(content_targets.keys()))\n",
    "\n",
    "# ---------------- TRAINABLE IMAGE ----------------\n",
    "input_img = content_img.clone().requires_grad_(True)\n",
    "\n",
    "# ---------------- CHECKPOINT DIR ----------------\n",
    "content_name = os.path.splitext(os.path.basename(content_image_path))[0]\n",
    "style_name   = os.path.splitext(os.path.basename(style_image_path))[0]\n",
    "checkpoint_dir = os.path.join(output_dir, f\"{content_name}__{style_name}\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "print(\"Checkpoints saved to:\", checkpoint_dir)\n",
    "\n",
    "# ---------------- OPTIMIZER / HYPERPARAMS ----------------\n",
    "style_weight = 1e4\n",
    "content_weight = 1.0\n",
    "max_steps = 1000\n",
    "\n",
    "# LBFGS (set max_iter to control upper bound)\n",
    "optimizer = optim.LBFGS([input_img], lr=1.0, max_iter=max_steps)\n",
    "\n",
    "save_steps = {50, 100, 150, 200, 250, 300}\n",
    "step_counter = [0]\n",
    "\n",
    "# ---------------- TRAINING LOOP ----------------\n",
    "print(\"Starting style transfer...\")\n",
    "\n",
    "def closure():\n",
    "    with torch.enable_grad():\n",
    "        optimizer.zero_grad()\n",
    "        x = input_img\n",
    "        style_loss = 0.0\n",
    "        content_loss = 0.0\n",
    "\n",
    "        # forward through full model, check mapping at indices\n",
    "        for idx, layer in enumerate(model):\n",
    "            x = layer(x)\n",
    "            if idx in layer_mapping:\n",
    "                name = layer_mapping[idx]\n",
    "                if name in content_layers:\n",
    "                    content_loss = content_loss + nn.functional.mse_loss(x, content_targets[name])\n",
    "                if name in style_layers:\n",
    "                    g = gram_matrix(x)\n",
    "                    style_loss = style_loss + nn.functional.mse_loss(g, style_targets[name])\n",
    "\n",
    "        loss = content_weight * content_loss + style_weight * style_loss\n",
    "        loss.backward()\n",
    "\n",
    "        step_counter[0] += 1\n",
    "        s = step_counter[0]\n",
    "\n",
    "        if s % 50 == 0:\n",
    "            print(f\"[{s:3d}] Content={content_loss.item():.6f}, Style={style_loss.item():.6f}\")\n",
    "            out = denormalize(input_img.detach())\n",
    "            save_image(out, os.path.join(checkpoint_dir, f\"step_{s}.png\"))\n",
    "            print(\"Saved:\", os.path.join(checkpoint_dir, f\"step_{s}.png\"))\n",
    "\n",
    "        return loss\n",
    "\n",
    "# optimizer.step(closure)\n",
    "#\n",
    "# # ---------------- FINAL ----------------\n",
    "# final_out = denormalize(input_img.detach())\n",
    "# save_image(final_out, os.path.join(checkpoint_dir, \"final.png\"))\n",
    "# print(\"Finished. Final image:\", os.path.join(checkpoint_dir, \"final.png\"))\n"
   ],
   "id": "9d469d8d6ff3a076",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured style target layers: ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n",
      "Captured content target layers: ['conv4_2']\n",
      "Checkpoints saved to: output_nst\\nst-dancing__nst-kadinsky\n",
      "Starting style transfer...\n",
      "[ 50] Content=20.755108, Style=0.001997\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_50.png\n",
      "[100] Content=15.333860, Style=0.001262\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_100.png\n",
      "[150] Content=13.460974, Style=0.001040\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_150.png\n",
      "[200] Content=12.443685, Style=0.000938\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_200.png\n",
      "[250] Content=11.852937, Style=0.000882\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_250.png\n",
      "[300] Content=11.456408, Style=0.000846\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_300.png\n",
      "[350] Content=11.180655, Style=0.000821\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_350.png\n",
      "[400] Content=10.974371, Style=0.000802\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_400.png\n",
      "[450] Content=10.808744, Style=0.000789\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_450.png\n",
      "[500] Content=10.684353, Style=0.000778\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_500.png\n",
      "[550] Content=10.582277, Style=0.000769\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_550.png\n",
      "[600] Content=10.496966, Style=0.000761\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_600.png\n",
      "[650] Content=10.420601, Style=0.000755\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_650.png\n",
      "[700] Content=10.356642, Style=0.000749\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_700.png\n",
      "[750] Content=10.297741, Style=0.000744\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_750.png\n",
      "[800] Content=10.245321, Style=0.000740\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_800.png\n",
      "[850] Content=10.201315, Style=0.000736\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_850.png\n",
      "[900] Content=10.162148, Style=0.000732\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_900.png\n",
      "[950] Content=10.125418, Style=0.000729\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_950.png\n",
      "[1000] Content=10.090732, Style=0.000727\n",
      "Saved: output_nst\\nst-dancing__nst-kadinsky\\step_1000.png\n",
      "Finished. Final image: output_nst\\nst-dancing__nst-kadinsky\\final.png\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:00:58.938277Z",
     "start_time": "2025-11-21T16:22:33.959957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------- BATCH PROCESSING ----------------\n",
    "import random\n",
    "\n",
    "def run_style_transfer(content_path, style_path, output_base_dir=\"output_nst\",\n",
    "                       batch_name=\"batch\", steps=1000, checkpoint_interval=50,\n",
    "                       style_weight_val=1e4, content_weight_val=1.0):\n",
    "    \"\"\"\n",
    "    Run style transfer on a single content/style pair with checkpoint saving.\n",
    "\n",
    "    Args:\n",
    "        content_path: Path to content image\n",
    "        style_path: Path to style image\n",
    "        output_base_dir: Base output directory (e.g., \"output_nst\")\n",
    "        batch_name: Batch subfolder name (e.g., \"batch\")\n",
    "        steps: Maximum optimization steps\n",
    "        checkpoint_interval: Save checkpoint every N steps (default: 50)\n",
    "        style_weight_val: Weight for style loss\n",
    "        content_weight_val: Weight for content loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Load images\n",
    "    content = load_img(content_path)\n",
    "    style = load_img(style_path)\n",
    "\n",
    "    # Extract style targets\n",
    "    style_targets_new = {}\n",
    "    x = style.clone()\n",
    "    for idx, layer in enumerate(model):\n",
    "        x = layer(x)\n",
    "        if idx in layer_mapping:\n",
    "            name = layer_mapping[idx]\n",
    "            if name in style_layers:\n",
    "                style_targets_new[name] = gram_matrix(x).detach()\n",
    "\n",
    "    # Extract content targets\n",
    "    content_targets_new = {}\n",
    "    y = content.clone()\n",
    "    for idx, layer in enumerate(model):\n",
    "        y = layer(y)\n",
    "        if idx in layer_mapping:\n",
    "            name = layer_mapping[idx]\n",
    "            if name in content_layers:\n",
    "                content_targets_new[name] = y.detach()\n",
    "\n",
    "    # Setup checkpoint directory: output_nst/batch/content__style/\n",
    "    c_name = os.path.splitext(os.path.basename(content_path))[0]\n",
    "    s_name = os.path.splitext(os.path.basename(style_path))[0]\n",
    "    ckpt_dir = os.path.join(output_base_dir, batch_name, f\"{c_name}__{s_name}\")\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize from content image\n",
    "    input_img = content.clone().requires_grad_(True)\n",
    "    optimizer = optim.LBFGS([input_img], lr=1.0, max_iter=steps)\n",
    "    step_counter = [0]\n",
    "\n",
    "    print(f\"\\nProcessing: {c_name} √ó {s_name}\")\n",
    "    print(f\"Output directory: {ckpt_dir}\")\n",
    "\n",
    "    # Closure function with checkpoint saving\n",
    "    def closure():\n",
    "        with torch.enable_grad():\n",
    "            optimizer.zero_grad()\n",
    "            x = input_img\n",
    "            style_loss = 0.0\n",
    "            content_loss = 0.0\n",
    "\n",
    "            for idx, layer in enumerate(model):\n",
    "                x = layer(x)\n",
    "                if idx in layer_mapping:\n",
    "                    name = layer_mapping[idx]\n",
    "                    if name in content_layers:\n",
    "                        content_loss = content_loss + nn.functional.mse_loss(x, content_targets_new[name])\n",
    "                    if name in style_layers:\n",
    "                        g = gram_matrix(x)\n",
    "                        style_loss = style_loss + nn.functional.mse_loss(g, style_targets_new[name])\n",
    "\n",
    "            loss = content_weight_val * content_loss + style_weight_val * style_loss\n",
    "            loss.backward()\n",
    "\n",
    "            step_counter[0] += 1\n",
    "            s = step_counter[0]\n",
    "\n",
    "            # Save checkpoints at specified intervals\n",
    "            if s % checkpoint_interval == 0:\n",
    "                print(f\"[{s:4d}] Content={content_loss.item():.6f}, Style={style_loss.item():.6f}\")\n",
    "                out = denormalize(input_img.detach())\n",
    "                save_image(out, os.path.join(ckpt_dir, f\"step_{s:04d}.png\"))\n",
    "\n",
    "            return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    # Save final result\n",
    "    final = denormalize(input_img.detach())\n",
    "    save_image(final, os.path.join(ckpt_dir, \"final.png\"))\n",
    "    print(f\"‚úì Saved final image: {ckpt_dir}/final.png\")\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "# ---------------- BATCH RUNNER WITH RANDOM STYLE SELECTION ----------------\n",
    "def run_batch_style_transfer(content_images, style_images, output_base_dir=\"output_nst\",\n",
    "                            batch_name=\"batch\", steps=500, checkpoint_interval=50):\n",
    "    \"\"\"\n",
    "    Run style transfer for multiple content images with random style selection.\n",
    "\n",
    "    Args:\n",
    "        content_images: List of content image paths\n",
    "        style_images: List of style image paths\n",
    "        output_base_dir: Base output directory\n",
    "        batch_name: Batch subfolder name\n",
    "        steps: Maximum optimization steps per image\n",
    "        checkpoint_interval: Save checkpoint every N steps\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"BATCH STYLE TRANSFER\")\n",
    "    print(f\"Content images: {len(content_images)}\")\n",
    "    print(f\"Style images: {len(style_images)}\")\n",
    "    print(f\"Output: {output_base_dir}/{batch_name}/\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for i, content_path in enumerate(content_images, 1):\n",
    "        # Randomly select a style for this content image\n",
    "        style_path = random.choice(style_images)\n",
    "\n",
    "        print(f\"\\n[{i}/{len(content_images)}] Content: {os.path.basename(content_path)}\")\n",
    "        print(f\"           Style: {os.path.basename(style_path)}\")\n",
    "\n",
    "        run_style_transfer(\n",
    "            content_path=content_path,\n",
    "            style_path=style_path,\n",
    "            output_base_dir=output_base_dir,\n",
    "            batch_name=batch_name,\n",
    "            steps=steps,\n",
    "            checkpoint_interval=checkpoint_interval\n",
    "        )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úì ALL TRANSFERS COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "# ---------------- DEFINE YOUR IMAGES ----------------\n",
    "\n",
    "content_images_generated = [\n",
    "    # DCGAN faces\n",
    "    \"Dataset/generated/dcgan_faces/dcgan_face_1.png\",\n",
    "    \"Dataset/generated/dcgan_faces/dcgan_face_2.png\",\n",
    "    \"Dataset/generated/dcgan_faces/dcgan_face_3.png\",\n",
    "    # CycleGAN Apple2Orange transformations\n",
    "    \"Dataset/generated/cyclegan_apple2orange/apple2orange_a2b.png\",\n",
    "    \"Dataset/generated/cyclegan_apple2orange/apple2orange_b2a.png\",\n",
    "    # CycleGAN Summer2Winter transformations\n",
    "    \"Dataset/generated/cyclegan_summer2winter/summer2winter_a2b.png\",\n",
    "    \"Dataset/generated/cyclegan_summer2winter/summer2winter_b2a.png\",\n",
    "]\n",
    "\n",
    "style_images = [\n",
    "    \"Dataset/NST/nst-picasso.jpg\",\n",
    "    \"Dataset/NST/nst-kadinsky.jpg\",\n",
    "]\n",
    "\n",
    "# Choose which content images to use\n",
    "content_images = content_images_generated\n",
    "\n",
    "# Filter out any files that don't exist\n",
    "content_images = [path for path in content_images if os.path.exists(path)]\n",
    "\n",
    "print(f\"\\nüìä Content images to process: {len(content_images)}\")\n",
    "print(f\"üé® Style images available: {len(style_images)}\")\n",
    "\n",
    "# Run batch processing with random style selection\n",
    "run_batch_style_transfer(\n",
    "    content_images=content_images,\n",
    "    style_images=style_images,\n",
    "    output_base_dir=\"output_nst\",\n",
    "    batch_name=\"batch\",\n",
    "    steps=500,\n",
    "    checkpoint_interval=50\n",
    ")"
   ],
   "id": "7052443452fa6be0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Content images to process: 7\n",
      "üé® Style images available: 2\n",
      "\n",
      "============================================================\n",
      "BATCH STYLE TRANSFER\n",
      "Content images: 7\n",
      "Style images: 2\n",
      "Output: output_nst/batch/\n",
      "============================================================\n",
      "\n",
      "[1/7] Content: dcgan_face_1.png\n",
      "           Style: nst-picasso.jpg\n",
      "\n",
      "Processing: dcgan_face_1 √ó nst-picasso\n",
      "Output directory: output_nst\\batch\\dcgan_face_1__nst-picasso\n",
      "[  50] Content=4.508502, Style=0.000311\n",
      "[ 100] Content=3.822603, Style=0.000254\n",
      "[ 150] Content=3.566385, Style=0.000232\n",
      "[ 200] Content=3.431097, Style=0.000220\n",
      "[ 250] Content=3.339631, Style=0.000213\n",
      "[ 300] Content=3.274787, Style=0.000207\n",
      "[ 350] Content=3.225577, Style=0.000202\n",
      "[ 400] Content=3.183739, Style=0.000199\n",
      "[ 450] Content=3.147371, Style=0.000196\n",
      "[ 500] Content=3.115758, Style=0.000193\n",
      "‚úì Saved final image: output_nst\\batch\\dcgan_face_1__nst-picasso/final.png\n",
      "\n",
      "[2/7] Content: dcgan_face_2.png\n",
      "           Style: nst-kadinsky.jpg\n",
      "\n",
      "Processing: dcgan_face_2 √ó nst-kadinsky\n",
      "Output directory: output_nst\\batch\\dcgan_face_2__nst-kadinsky\n",
      "[  50] Content=26.995358, Style=0.002137\n",
      "[ 100] Content=20.514341, Style=0.001257\n",
      "[ 150] Content=18.118923, Style=0.001002\n",
      "[ 200] Content=16.819233, Style=0.000890\n",
      "[ 250] Content=16.017647, Style=0.000834\n",
      "[ 300] Content=15.497224, Style=0.000799\n",
      "[ 350] Content=15.106373, Style=0.000777\n",
      "[ 400] Content=14.818290, Style=0.000762\n",
      "[ 450] Content=14.592318, Style=0.000751\n",
      "[ 500] Content=14.403591, Style=0.000743\n",
      "‚úì Saved final image: output_nst\\batch\\dcgan_face_2__nst-kadinsky/final.png\n",
      "\n",
      "[3/7] Content: dcgan_face_3.png\n",
      "           Style: nst-picasso.jpg\n",
      "\n",
      "Processing: dcgan_face_3 √ó nst-picasso\n",
      "Output directory: output_nst\\batch\\dcgan_face_3__nst-picasso\n",
      "[  50] Content=4.315878, Style=0.000333\n",
      "[ 100] Content=3.602579, Style=0.000255\n",
      "[ 150] Content=3.333574, Style=0.000228\n",
      "[ 200] Content=3.189144, Style=0.000214\n",
      "[ 250] Content=3.089321, Style=0.000205\n",
      "[ 300] Content=3.018019, Style=0.000198\n",
      "[ 350] Content=2.966142, Style=0.000193\n",
      "[ 400] Content=2.925278, Style=0.000189\n",
      "[ 450] Content=2.892296, Style=0.000186\n",
      "[ 500] Content=2.865841, Style=0.000183\n",
      "‚úì Saved final image: output_nst\\batch\\dcgan_face_3__nst-picasso/final.png\n",
      "\n",
      "[4/7] Content: apple2orange_a2b.png\n",
      "           Style: nst-picasso.jpg\n",
      "\n",
      "Processing: apple2orange_a2b √ó nst-picasso\n",
      "Output directory: output_nst\\batch\\apple2orange_a2b__nst-picasso\n",
      "[  50] Content=3.672875, Style=0.000766\n",
      "[ 100] Content=3.116940, Style=0.000661\n",
      "[ 150] Content=2.864618, Style=0.000594\n",
      "[ 200] Content=2.716957, Style=0.000535\n",
      "[ 250] Content=2.628562, Style=0.000480\n",
      "[ 300] Content=2.569552, Style=0.000434\n",
      "[ 350] Content=2.523440, Style=0.000397\n",
      "[ 400] Content=2.479594, Style=0.000370\n",
      "[ 450] Content=2.440987, Style=0.000350\n",
      "[ 500] Content=2.409060, Style=0.000335\n",
      "‚úì Saved final image: output_nst\\batch\\apple2orange_a2b__nst-picasso/final.png\n",
      "\n",
      "[5/7] Content: apple2orange_b2a.png\n",
      "           Style: nst-picasso.jpg\n",
      "\n",
      "Processing: apple2orange_b2a √ó nst-picasso\n",
      "Output directory: output_nst\\batch\\apple2orange_b2a__nst-picasso\n",
      "[  50] Content=5.399012, Style=0.001170\n",
      "[ 100] Content=4.639835, Style=0.000641\n",
      "[ 150] Content=4.239637, Style=0.000404\n",
      "[ 200] Content=3.982028, Style=0.000323\n",
      "[ 250] Content=3.838411, Style=0.000293\n",
      "[ 300] Content=3.750464, Style=0.000278\n",
      "[ 350] Content=3.689473, Style=0.000269\n",
      "[ 400] Content=3.644152, Style=0.000262\n",
      "[ 450] Content=3.610578, Style=0.000257\n",
      "[ 500] Content=3.584142, Style=0.000253\n",
      "‚úì Saved final image: output_nst\\batch\\apple2orange_b2a__nst-picasso/final.png\n",
      "\n",
      "[6/7] Content: summer2winter_a2b.png\n",
      "           Style: nst-kadinsky.jpg\n",
      "\n",
      "Processing: summer2winter_a2b √ó nst-kadinsky\n",
      "Output directory: output_nst\\batch\\summer2winter_a2b__nst-kadinsky\n",
      "[  50] Content=22.479681, Style=0.001962\n",
      "[ 100] Content=17.145828, Style=0.001225\n",
      "[ 150] Content=15.116709, Style=0.001012\n",
      "[ 200] Content=13.992512, Style=0.000922\n",
      "[ 250] Content=13.358784, Style=0.000868\n",
      "[ 300] Content=12.913723, Style=0.000834\n",
      "[ 350] Content=12.586330, Style=0.000811\n",
      "[ 400] Content=12.350890, Style=0.000793\n",
      "[ 450] Content=12.157970, Style=0.000781\n",
      "[ 500] Content=11.998028, Style=0.000770\n",
      "‚úì Saved final image: output_nst\\batch\\summer2winter_a2b__nst-kadinsky/final.png\n",
      "\n",
      "[7/7] Content: summer2winter_b2a.png\n",
      "           Style: nst-picasso.jpg\n",
      "\n",
      "Processing: summer2winter_b2a √ó nst-picasso\n",
      "Output directory: output_nst\\batch\\summer2winter_b2a__nst-picasso\n",
      "[  50] Content=2.482106, Style=0.000240\n",
      "[ 100] Content=2.129726, Style=0.000193\n",
      "[ 150] Content=1.981369, Style=0.000175\n",
      "[ 200] Content=1.900663, Style=0.000165\n",
      "[ 250] Content=1.846057, Style=0.000159\n",
      "[ 300] Content=1.811447, Style=0.000154\n",
      "[ 350] Content=1.784357, Style=0.000151\n",
      "[ 400] Content=1.762999, Style=0.000148\n",
      "[ 450] Content=1.746562, Style=0.000146\n",
      "[ 500] Content=1.732069, Style=0.000144\n",
      "‚úì Saved final image: output_nst\\batch\\summer2winter_b2a__nst-picasso/final.png\n",
      "\n",
      "============================================================\n",
      "‚úì ALL TRANSFERS COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:22:04.884890Z",
     "start_time": "2025-11-21T17:00:59.103313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# APPLY STYLE TRANSFER TO CYCLEGAN PAIRS AND CREATE COMPARISONS\n",
    "# ============================================================================\n",
    "\n",
    "def stylize_and_compare_cyclegan_pairs(style_path, output_base_dir=\"output_nst\",\n",
    "                                       batch_name=\"cyclegan_comparisons\",\n",
    "                                       steps=500, checkpoint_interval=50):\n",
    "    \"\"\"\n",
    "    Apply style transfer to CycleGAN pair images and create side-by-side comparisons.\n",
    "\n",
    "    For each model (apple2orange, summer2winter):\n",
    "    1. Apply style to original image (pair_a)\n",
    "    2. Apply style to generated image (pair_b)\n",
    "    3. Concatenate them horizontally for comparison\n",
    "    \"\"\"\n",
    "    import torch\n",
    "\n",
    "    pairs = [\n",
    "        {\n",
    "            \"name\": \"apple2orange\",\n",
    "            \"pair_a\": \"Dataset/generated/cyclegan_apple2orange/apple2orange_pair_a.png\",\n",
    "            \"pair_b\": \"Dataset/generated/cyclegan_apple2orange/apple2orange_pair_b.png\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"summer2winter\",\n",
    "            \"pair_a\": \"Dataset/generated/cyclegan_summer2winter/summer2winter_pair_a.png\",\n",
    "            \"pair_b\": \"Dataset/generated/cyclegan_summer2winter/summer2winter_pair_b.png\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"APPLYING STYLE TRANSFER TO CYCLEGAN PAIRS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for pair_info in pairs:\n",
    "        name = pair_info[\"name\"]\n",
    "        pair_a_path = pair_info[\"pair_a\"]\n",
    "        pair_b_path = pair_info[\"pair_b\"]\n",
    "\n",
    "        if not os.path.exists(pair_a_path) or not os.path.exists(pair_b_path):\n",
    "            print(f\"\\n‚ö†Ô∏è  Skipping {name} - files not found\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing pair: {name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Apply style to original (A)\n",
    "        print(f\"\\n[1/2] Stylizing original image...\")\n",
    "        styled_a_tensor = run_style_transfer(\n",
    "            content_path=pair_a_path,\n",
    "            style_path=style_path,\n",
    "            output_base_dir=output_base_dir,\n",
    "            batch_name=f\"{batch_name}/{name}\",\n",
    "            steps=steps,\n",
    "            checkpoint_interval=checkpoint_interval\n",
    "        )\n",
    "\n",
    "        # Apply style to generated (B)\n",
    "        print(f\"\\n[2/2] Stylizing generated image...\")\n",
    "        styled_b_tensor = run_style_transfer(\n",
    "            content_path=pair_b_path,\n",
    "            style_path=style_path,\n",
    "            output_base_dir=output_base_dir,\n",
    "            batch_name=f\"{batch_name}/{name}\",\n",
    "            steps=steps,\n",
    "            checkpoint_interval=checkpoint_interval\n",
    "        )\n",
    "\n",
    "        # Load the saved styled images\n",
    "        styled_a_dir = os.path.join(output_base_dir, f\"{batch_name}/{name}\",\n",
    "                                     f\"{os.path.splitext(os.path.basename(pair_a_path))[0]}__{os.path.splitext(os.path.basename(style_path))[0]}\")\n",
    "        styled_b_dir = os.path.join(output_base_dir, f\"{batch_name}/{name}\",\n",
    "                                     f\"{os.path.splitext(os.path.basename(pair_b_path))[0]}__{os.path.splitext(os.path.basename(style_path))[0]}\")\n",
    "\n",
    "        styled_a_img_path = os.path.join(styled_a_dir, \"final.png\")\n",
    "        styled_b_img_path = os.path.join(styled_b_dir, \"final.png\")\n",
    "\n",
    "        # Load and concatenate\n",
    "        if os.path.exists(styled_a_img_path) and os.path.exists(styled_b_img_path):\n",
    "            from torchvision import io\n",
    "            styled_a_img = io.read_image(styled_a_img_path).float() / 255.0\n",
    "            styled_b_img = io.read_image(styled_b_img_path).float() / 255.0\n",
    "\n",
    "            # Concatenate horizontally (side-by-side)\n",
    "            comparison = torch.cat([styled_a_img, styled_b_img], dim=2)  # dim=2 is width\n",
    "\n",
    "            # Save comparison\n",
    "            comparison_dir = os.path.join(output_base_dir, batch_name, name)\n",
    "            os.makedirs(comparison_dir, exist_ok=True)\n",
    "            comparison_path = os.path.join(comparison_dir, f\"{name}_styled_comparison.png\")\n",
    "\n",
    "            save_image(comparison, comparison_path)\n",
    "            print(f\"\\n‚úÖ Saved comparison: {comparison_path}\")\n",
    "            print(f\"   (Original styled | Generated styled) - {comparison.shape[2]}x{comparison.shape[1]} px\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  Could not find styled images for comparison\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ ALL COMPARISONS COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "# Example usage - uncomment and run when ready:\n",
    "stylize_and_compare_cyclegan_pairs(\n",
    "    style_path=\"Dataset/NST/nst-kadinsky.jpg\",\n",
    "    output_base_dir=\"output_nst\",\n",
    "    batch_name=\"cyclegan_comparisons\",\n",
    "    steps=500,\n",
    "    checkpoint_interval=50\n",
    ")\n"
   ],
   "id": "105b5d69a2bd7d9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "APPLYING STYLE TRANSFER TO CYCLEGAN PAIRS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Processing pair: apple2orange\n",
      "============================================================\n",
      "\n",
      "[1/2] Stylizing original image...\n",
      "\n",
      "Processing: apple2orange_pair_a √ó nst-kadinsky\n",
      "Output directory: output_nst\\cyclegan_comparisons/apple2orange\\apple2orange_pair_a__nst-kadinsky\n",
      "[  50] Content=25.404797, Style=0.002440\n",
      "[ 100] Content=18.381714, Style=0.001537\n",
      "[ 150] Content=15.861139, Style=0.001259\n",
      "[ 200] Content=14.541812, Style=0.001135\n",
      "[ 250] Content=13.725191, Style=0.001065\n",
      "[ 300] Content=13.182171, Style=0.001020\n",
      "[ 350] Content=12.791217, Style=0.000988\n",
      "[ 400] Content=12.479332, Style=0.000966\n",
      "[ 450] Content=12.229878, Style=0.000948\n",
      "[ 500] Content=12.028555, Style=0.000934\n",
      "‚úì Saved final image: output_nst\\cyclegan_comparisons/apple2orange\\apple2orange_pair_a__nst-kadinsky/final.png\n",
      "\n",
      "[2/2] Stylizing generated image...\n",
      "\n",
      "Processing: apple2orange_pair_b √ó nst-kadinsky\n",
      "Output directory: output_nst\\cyclegan_comparisons/apple2orange\\apple2orange_pair_b__nst-kadinsky\n",
      "[  50] Content=27.426661, Style=0.003057\n",
      "[ 100] Content=20.015461, Style=0.001791\n",
      "[ 150] Content=17.332909, Style=0.001388\n",
      "[ 200] Content=15.814288, Style=0.001180\n",
      "[ 250] Content=14.837227, Style=0.001057\n",
      "[ 300] Content=14.148726, Style=0.000983\n",
      "[ 350] Content=13.653948, Style=0.000939\n",
      "[ 400] Content=13.270630, Style=0.000909\n",
      "[ 450] Content=12.973166, Style=0.000889\n",
      "[ 500] Content=12.746183, Style=0.000874\n",
      "‚úì Saved final image: output_nst\\cyclegan_comparisons/apple2orange\\apple2orange_pair_b__nst-kadinsky/final.png\n",
      "\n",
      "‚úÖ Saved comparison: output_nst\\cyclegan_comparisons\\apple2orange\\apple2orange_styled_comparison.png\n",
      "   (Original styled | Generated styled) - 1024x512 px\n",
      "\n",
      "============================================================\n",
      "Processing pair: summer2winter\n",
      "============================================================\n",
      "\n",
      "[1/2] Stylizing original image...\n",
      "\n",
      "Processing: summer2winter_pair_a √ó nst-kadinsky\n",
      "Output directory: output_nst\\cyclegan_comparisons/summer2winter\\summer2winter_pair_a__nst-kadinsky\n",
      "[  50] Content=20.659386, Style=0.001564\n",
      "[ 100] Content=16.201426, Style=0.001059\n",
      "[ 150] Content=14.317296, Style=0.000900\n",
      "[ 200] Content=13.374523, Style=0.000824\n",
      "[ 250] Content=12.773700, Style=0.000782\n",
      "[ 300] Content=12.390186, Style=0.000753\n",
      "[ 350] Content=12.114283, Style=0.000733\n",
      "[ 400] Content=11.892353, Style=0.000718\n",
      "[ 450] Content=11.724554, Style=0.000707\n",
      "[ 500] Content=11.584520, Style=0.000698\n",
      "‚úì Saved final image: output_nst\\cyclegan_comparisons/summer2winter\\summer2winter_pair_a__nst-kadinsky/final.png\n",
      "\n",
      "[2/2] Stylizing generated image...\n",
      "\n",
      "Processing: summer2winter_pair_b √ó nst-kadinsky\n",
      "Output directory: output_nst\\cyclegan_comparisons/summer2winter\\summer2winter_pair_b__nst-kadinsky\n",
      "[  50] Content=21.597231, Style=0.001525\n",
      "[ 100] Content=16.506390, Style=0.001029\n",
      "[ 150] Content=14.559336, Style=0.000878\n",
      "[ 200] Content=13.539696, Style=0.000810\n",
      "[ 250] Content=12.931685, Style=0.000771\n",
      "[ 300] Content=12.522818, Style=0.000747\n",
      "[ 350] Content=12.225449, Style=0.000730\n",
      "[ 400] Content=11.999416, Style=0.000718\n",
      "[ 450] Content=11.827913, Style=0.000709\n",
      "[ 500] Content=11.692076, Style=0.000701\n",
      "‚úì Saved final image: output_nst\\cyclegan_comparisons/summer2winter\\summer2winter_pair_b__nst-kadinsky/final.png\n",
      "\n",
      "‚úÖ Saved comparison: output_nst\\cyclegan_comparisons\\summer2winter\\summer2winter_styled_comparison.png\n",
      "   (Original styled | Generated styled) - 1024x512 px\n",
      "\n",
      "============================================================\n",
      "‚úÖ ALL COMPARISONS COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T16:20:04.885239Z",
     "start_time": "2025-11-21T16:19:48.504499Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® üé® \n",
      "\n",
      "============================================================\n",
      "GENERATING DCGAN IMAGES\n",
      "============================================================\n",
      "‚úÖ Loaded checkpoint: output_dcgan\\checkpoint_epoch_500.pth (epoch 500)\n",
      "‚úÖ Loaded generator from epoch 500\n",
      "   [1/3] Saved: Dataset/generated/dcgan_faces\\dcgan_face_1.png\n",
      "   [2/3] Saved: Dataset/generated/dcgan_faces\\dcgan_face_2.png\n",
      "   [3/3] Saved: Dataset/generated/dcgan_faces\\dcgan_face_3.png\n",
      "‚úÖ Generated 3 DCGAN face images\n",
      "\n",
      "üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé üçé \n",
      "\n",
      "============================================================\n",
      "GENERATING CycleGAN IMAGES - apple2orange\n",
      "============================================================\n",
      "‚úÖ Loaded checkpoint: output_cyclegan_a2o\\checkpoint_epoch_50.pth (epoch 50)\n",
      "‚úÖ Loaded generators from epoch 50\n",
      "\n",
      "[1/3] Generating A‚ÜíB transformation...\n",
      "   Saved: Dataset/generated/cyclegan_apple2orange\\apple2orange_a2b.png\n",
      "\n",
      "[2/3] Generating B‚ÜíA transformation...\n",
      "   Saved: Dataset/generated/cyclegan_apple2orange\\apple2orange_b2a.png\n",
      "\n",
      "[3/3] Generating A‚ÜíB comparison pair...\n",
      "   Saved original A: Dataset/generated/cyclegan_apple2orange\\apple2orange_pair_a.png\n",
      "   Saved generated B: Dataset/generated/cyclegan_apple2orange\\apple2orange_pair_b.png\n",
      "   (These will be styled separately then concatenated for comparison)\n",
      "\n",
      "‚úÖ Generated 4 CycleGAN images (2 singles + 1 pair)\n",
      "\n",
      "‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è ‚ùÑÔ∏è \n",
      "\n",
      "============================================================\n",
      "GENERATING CycleGAN IMAGES - summer2winter\n",
      "============================================================\n",
      "‚úÖ Loaded checkpoint: output_cyclegan_s2w\\checkpoint_epoch_50.pth (epoch 50)\n",
      "‚úÖ Loaded generators from epoch 50\n",
      "\n",
      "[1/3] Generating A‚ÜíB transformation...\n",
      "   Saved: Dataset/generated/cyclegan_summer2winter\\summer2winter_a2b.png\n",
      "\n",
      "[2/3] Generating B‚ÜíA transformation...\n",
      "   Saved: Dataset/generated/cyclegan_summer2winter\\summer2winter_b2a.png\n",
      "\n",
      "[3/3] Generating A‚ÜíB comparison pair...\n",
      "   Saved original A: Dataset/generated/cyclegan_summer2winter\\summer2winter_pair_a.png\n",
      "   Saved generated B: Dataset/generated/cyclegan_summer2winter\\summer2winter_pair_b.png\n",
      "   (These will be styled separately then concatenated for comparison)\n",
      "\n",
      "‚úÖ Generated 4 CycleGAN images (2 singles + 1 pair)\n",
      "\n",
      "============================================================\n",
      "‚úÖ ALL IMAGE GENERATION COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Total files generated: 11\n"
     ]
    }
   ],
   "execution_count": 11,
   "source": [
    "# ============================================================================\n",
    "# GENERATE IMAGES FROM TRAINED MODELS FOR NST CONTENT INPUT\n",
    "# ============================================================================\n",
    "import torch\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------------- DCGAN IMAGE GENERATION ----------------\n",
    "def generate_dcgan_images(checkpoint_dir, output_dir, num_samples=3, noise_size=100):\n",
    "    \"\"\"\n",
    "    Generate 3 random face images from trained DCGAN model.\n",
    "\n",
    "    Outputs:\n",
    "        - dcgan_face_1.png\n",
    "        - dcgan_face_2.png\n",
    "        - dcgan_face_3.png\n",
    "    \"\"\"\n",
    "    from Model import DCGAN_Generator, load_checkpoint_generic\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GENERATING DCGAN IMAGES\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load checkpoint\n",
    "    checkpoint = load_checkpoint_generic(checkpoint_dir, device)\n",
    "    if not checkpoint:\n",
    "        print(f\"‚ùå No checkpoint found in {checkpoint_dir}\")\n",
    "        return []\n",
    "\n",
    "    # Initialize generator\n",
    "    generator = DCGAN_Generator(z_dim=noise_size, img_channels=3).to(device)\n",
    "    generator.load_state_dict(checkpoint['generator'])\n",
    "    generator.eval()\n",
    "\n",
    "    print(f\"‚úÖ Loaded generator from epoch {checkpoint.get('epoch', '?')}\")\n",
    "\n",
    "    generated_files = []\n",
    "\n",
    "    # Generate 3 random face images\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            noise = torch.randn(1, noise_size, 1, 1, device=device)\n",
    "            fake_img = generator(noise)\n",
    "\n",
    "            # Denormalize from [-1, 1] to [0, 1]\n",
    "            fake_img = (fake_img + 1) / 2.0\n",
    "\n",
    "            # Upscale to 1024x1024 for NST\n",
    "            upscale = transforms.Resize((1024, 1024), interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "            fake_img_large = upscale(fake_img)\n",
    "\n",
    "            output_path = os.path.join(output_dir, f\"dcgan_face_{i+1}.png\")\n",
    "            save_image(fake_img_large, output_path)\n",
    "            generated_files.append(output_path)\n",
    "            print(f\"   [{i+1}/3] Saved: {output_path}\")\n",
    "\n",
    "    print(f\"‚úÖ Generated {num_samples} DCGAN face images\")\n",
    "    del generator\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return generated_files\n",
    "\n",
    "\n",
    "# ---------------- CycleGAN IMAGE GENERATION ----------------\n",
    "def generate_cyclegan_images(checkpoint_dir, dataset_root, output_dir, model_name=\"cyclegan\"):\n",
    "    \"\"\"\n",
    "    Generate 3 content images from trained CycleGAN model:\n",
    "    1. A‚ÜíB transformation (single output)\n",
    "    2. B‚ÜíA transformation (single output)\n",
    "    3. A‚ÜíB pair (original A + generated B side-by-side for comparison)\n",
    "\n",
    "    Outputs:\n",
    "        - cyclegan_a2b.png       (transformed A‚ÜíB)\n",
    "        - cyclegan_b2a.png       (transformed B‚ÜíA)\n",
    "        - cyclegan_pair_a.png    (original A from testA)\n",
    "        - cyclegan_pair_b.png    (transformed A‚ÜíB for comparison)\n",
    "    \"\"\"\n",
    "    from Model import CycleGAN_Generator, load_checkpoint_generic\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GENERATING CycleGAN IMAGES - {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load checkpoint\n",
    "    checkpoint = load_checkpoint_generic(checkpoint_dir, device)\n",
    "    if not checkpoint:\n",
    "        print(f\"‚ùå No checkpoint found in {checkpoint_dir}\")\n",
    "        return []\n",
    "\n",
    "    # Initialize both generators\n",
    "    G_A2B = CycleGAN_Generator().to(device)\n",
    "    G_B2A = CycleGAN_Generator().to(device)\n",
    "\n",
    "    G_A2B.load_state_dict(checkpoint['G_A2B'])\n",
    "    G_B2A.load_state_dict(checkpoint['G_B2A'])\n",
    "\n",
    "    G_A2B.eval()\n",
    "    G_B2A.eval()\n",
    "\n",
    "    print(f\"‚úÖ Loaded generators from epoch {checkpoint.get('epoch', '?')}\")\n",
    "\n",
    "    # Load test images\n",
    "    test_dir_A = os.path.join(dataset_root, 'testA')\n",
    "    test_dir_B = os.path.join(dataset_root, 'testB')\n",
    "\n",
    "    test_images_A = sorted([f for f in os.listdir(test_dir_A) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    test_images_B = sorted([f for f in os.listdir(test_dir_B) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "\n",
    "    if len(test_images_A) == 0 or len(test_images_B) == 0:\n",
    "        print(f\"‚ùå No test images found in {test_dir_A} or {test_dir_B}\")\n",
    "        return []\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    upscale = transforms.Resize((1024, 1024), interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "\n",
    "    generated_files = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 1. Generate A‚ÜíB (single output)\n",
    "        print(f\"\\n[1/3] Generating A‚ÜíB transformation...\")\n",
    "        img_path_A = os.path.join(test_dir_A, test_images_A[0])\n",
    "        img_A = Image.open(img_path_A).convert('RGB')\n",
    "        img_A_tensor = transform(img_A).unsqueeze(0).to(device)\n",
    "\n",
    "        fake_B = G_A2B(img_A_tensor)\n",
    "        fake_B = (fake_B + 1) / 2.0  # Denormalize\n",
    "        fake_B_large = upscale(fake_B)\n",
    "\n",
    "        output_a2b = os.path.join(output_dir, f\"{model_name}_a2b.png\")\n",
    "        save_image(fake_B_large, output_a2b)\n",
    "        generated_files.append(output_a2b)\n",
    "        print(f\"   Saved: {output_a2b}\")\n",
    "\n",
    "        # 2. Generate B‚ÜíA (single output)\n",
    "        print(f\"\\n[2/3] Generating B‚ÜíA transformation...\")\n",
    "        img_path_B = os.path.join(test_dir_B, test_images_B[0])\n",
    "        img_B = Image.open(img_path_B).convert('RGB')\n",
    "        img_B_tensor = transform(img_B).unsqueeze(0).to(device)\n",
    "\n",
    "        fake_A = G_B2A(img_B_tensor)\n",
    "        fake_A = (fake_A + 1) / 2.0  # Denormalize\n",
    "        fake_A_large = upscale(fake_A)\n",
    "\n",
    "        output_b2a = os.path.join(output_dir, f\"{model_name}_b2a.png\")\n",
    "        save_image(fake_A_large, output_b2a)\n",
    "        generated_files.append(output_b2a)\n",
    "        print(f\"   Saved: {output_b2a}\")\n",
    "\n",
    "        # 3. Generate A‚ÜíB pair (original A + generated B for comparison)\n",
    "        print(f\"\\n[3/3] Generating A‚ÜíB comparison pair...\")\n",
    "        # Use a different image from testA for variety\n",
    "        img_path_A_pair = os.path.join(test_dir_A, test_images_A[1 % len(test_images_A)])\n",
    "        img_A_pair = Image.open(img_path_A_pair).convert('RGB')\n",
    "        img_A_pair_tensor = transform(img_A_pair).unsqueeze(0).to(device)\n",
    "\n",
    "        # Original A\n",
    "        img_A_pair_norm = (img_A_pair_tensor + 1) / 2.0  # Denormalize\n",
    "        img_A_pair_large = upscale(img_A_pair_norm)\n",
    "\n",
    "        # Generated B from A\n",
    "        fake_B_pair = G_A2B(img_A_pair_tensor)\n",
    "        fake_B_pair = (fake_B_pair + 1) / 2.0  # Denormalize\n",
    "        fake_B_pair_large = upscale(fake_B_pair)\n",
    "\n",
    "        # Save both separately for individual style transfer\n",
    "        output_pair_a = os.path.join(output_dir, f\"{model_name}_pair_a.png\")\n",
    "        output_pair_b = os.path.join(output_dir, f\"{model_name}_pair_b.png\")\n",
    "\n",
    "        save_image(img_A_pair_large, output_pair_a)\n",
    "        save_image(fake_B_pair_large, output_pair_b)\n",
    "\n",
    "        generated_files.append(output_pair_a)\n",
    "        generated_files.append(output_pair_b)\n",
    "\n",
    "        print(f\"   Saved original A: {output_pair_a}\")\n",
    "        print(f\"   Saved generated B: {output_pair_b}\")\n",
    "        print(f\"   (These will be styled separately then concatenated for comparison)\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Generated 4 CycleGAN images (2 singles + 1 pair)\")\n",
    "    del G_A2B, G_B2A\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return generated_files\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION - Generate all images\n",
    "# ============================================================================\n",
    "\n",
    "all_generated_files = []\n",
    "\n",
    "# 1. DCGAN - Generate 3 random faces\n",
    "print(\"\\n\" + \"üé® \"*30)\n",
    "dcgan_files = generate_dcgan_images(\n",
    "    checkpoint_dir=\"output_dcgan\",\n",
    "    output_dir=\"Dataset/generated/dcgan_faces\",\n",
    "    num_samples=3,\n",
    "    noise_size=100\n",
    ")\n",
    "all_generated_files.extend(dcgan_files)\n",
    "\n",
    "# 2. CycleGAN - Apple to Orange (1 A‚ÜíB, 1 B‚ÜíA, 1 pair)\n",
    "print(\"\\n\" + \"üçé \"*30)\n",
    "a2o_files = generate_cyclegan_images(\n",
    "    checkpoint_dir=\"output_cyclegan_a2o\",\n",
    "    dataset_root=\"Dataset/Apple2orange\",\n",
    "    output_dir=\"Dataset/generated/cyclegan_apple2orange\",\n",
    "    model_name=\"apple2orange\"\n",
    ")\n",
    "all_generated_files.extend(a2o_files)\n",
    "\n",
    "# 3. CycleGAN - Summer to Winter (1 A‚ÜíB, 1 B‚ÜíA, 1 pair)\n",
    "print(\"\\n\" + \"‚ùÑÔ∏è \"*30)\n",
    "s2w_files = generate_cyclegan_images(\n",
    "    checkpoint_dir=\"output_cyclegan_s2w\",\n",
    "    dataset_root=\"Dataset/Summer2Winter_Yosemite\",\n",
    "    output_dir=\"Dataset/generated/cyclegan_summer2winter\",\n",
    "    model_name=\"summer2winter\"\n",
    ")\n",
    "all_generated_files.extend(s2w_files)\n",
    "#\n",
    "# # ============================================================================\n",
    "# # SUMMARY\n",
    "# # ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ ALL IMAGE GENERATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal files generated: {len(all_generated_files)}\")\n"
   ],
   "id": "a8ffb1dfa52b2ad6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# V3.1 DCGAN output\n",
    "\n",
    "<img src=\"output_dcgan/loss_curves.png\"/>\n",
    "\n",
    "<!-- DCGAN Training Progress - Selected Epochs -->\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"output_dcgan/epoch_0100.png\" width=\"180\"/><br>\n",
    "    <small>Epoch 100</small>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"output_dcgan/epoch_0200.png\" width=\"180\"/><br>\n",
    "    <small>Epoch 200</small>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"output_dcgan/epoch_0300.png\" width=\"180\"/><br>\n",
    "    <small>Epoch 300</small>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"output_dcgan/epoch_0400.png\" width=\"180\"/><br>\n",
    "    <small>Epoch 400</small>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"output_dcgan/epoch_0500.png\" width=\"180\"/><br>\n",
    "    <small>Epoch 500</small>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<img src=\"output_dcgan/training_progress.gif\" width=\"1000\"/>\n"
   ],
   "id": "818f555b2171170f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# V3.2 CycleGAN - Apple2Orange\n",
    "\n",
    "## Loss Curves\n",
    "<img src=\"output_cyclegan_a2o/loss_curves.png\"/>\n",
    "\n",
    "## Training Progress - Selected Epochs\n",
    "<!-- Apple2Orange CycleGAN Training Progress -->\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"output_cyclegan_a2o/epoch_0010.png\" width=\"180\"/><br>\n",
    "    <small>Epoch 10</small>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"output_cyclegan_a2o/epoch_0020.png\" width=\"180\"/><br>\n",
    "    <small>Epoch 20</small>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"output_cyclegan_a2o/epoch_0030.png\" width=\"180\"/><br>\n",
    "    <small>Epoch 30</small>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"output_cyclegan_a2o/epoch_0040.png\" width=\"180\"/><br>\n",
    "    <small>Epoch 40</small>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"output_cyclegan_a2o/epoch_0050.png\" width=\"180\"/><br>\n",
    "    <small>Epoch 50</small>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "## Training Animation\n",
    "<img src=\"output_cyclegan_a2o/training_progress_50.gif\" width=\"800\"/>\n",
    "\n",
    "## Generated Samples\n",
    "\n",
    "### Apple ‚Üí Orange Transformation\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"Dataset/generated/cyclegan_apple2orange/apple2orange_pair_a.png\" width=\"300\"/><br>\n",
    "    <small>Original Apple</small>\n",
    "  </div>\n",
    "  <div style=\"text-align: center; font-size: 48px;\">‚Üí</div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"Dataset/generated/cyclegan_apple2orange/apple2orange_pair_b.png\" width=\"300\"/><br>\n",
    "    <small>Generated Orange</small>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "### Orange ‚Üí Apple Transformation\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"Dataset/generated/cyclegan_apple2orange/apple2orange_b2a.png\" width=\"300\"/><br>\n",
    "  <small>Orange ‚Üí Apple</small>\n",
    "</div>\n",
    "\n",
    "### Single Transformation Output\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"Dataset/generated/cyclegan_apple2orange/apple2orange_a2b.png\" width=\"300\"/><br>\n",
    "  <small>Apple ‚Üí Orange (Single Output)</small>\n",
    "</div>"
   ],
   "id": "a1b34072921dde4d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
