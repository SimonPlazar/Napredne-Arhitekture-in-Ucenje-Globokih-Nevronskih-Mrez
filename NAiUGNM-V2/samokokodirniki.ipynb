{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:50.393272Z",
     "start_time": "2025-11-05T21:46:36.330556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import numpy as np\n"
   ],
   "id": "dfdc2ddad2f876c9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:50.441766Z",
     "start_time": "2025-11-05T21:46:50.426289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Basic_Autoencoder(nn.Module):\n",
    "    def __init__(self, input_shape=(1, 28, 28), latent_size=32):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        # Calculate flattened input size\n",
    "        self.input_size = input_shape[0] * input_shape[1] * input_shape[2]\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_size)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        out = self.decoder(z)\n",
    "        return out.view(-1, *self.input_shape)"
   ],
   "id": "be6b7e686af64f2b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:50.483053Z",
     "start_time": "2025-11-05T21:46:50.467793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Deep_Autoencoder(nn.Module):\n",
    "    def __init__(self, input_shape=(1, 28, 28), latent_size=32):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        # Calculate flattened input size\n",
    "        self.input_size = input_shape[0] * input_shape[1] * input_shape[2]\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_size)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        out = self.decoder(z)\n",
    "        return out.view(-1, *self.input_shape)"
   ],
   "id": "ced7479de7f37a6e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:50.547299Z",
     "start_time": "2025-11-05T21:46:50.510084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self, input_size=28 * 28, latent_size=32):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc_mu = nn.Linear(512, latent_size)\n",
    "        self.fc_logvar = nn.Linear(512, latent_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        h = self.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "class Sampling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, mean, logvar):\n",
    "        batch_size, dim = mean.shape\n",
    "        epsilon = torch.randn(batch_size, dim, device=mean.device)\n",
    "        return mean + torch.exp(0.5 * logvar) * epsilon\n",
    "\n",
    "\n",
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self, latent_size=32, output_size=28 * 28, output_shape=(1, 28, 28)):\n",
    "        super().__init__()\n",
    "        self.output_shape = output_shape\n",
    "        self.fc1 = nn.Linear(latent_size, 512)\n",
    "        self.fc2 = nn.Linear(512, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.relu(self.fc1(z))\n",
    "        out = self.sigmoid(self.fc2(h))\n",
    "        return out.view(-1, *self.output_shape)\n",
    "\n",
    "\n",
    "class BetaVAE(nn.Module):\n",
    "    def __init__(self, input_shape=(1, 28, 28), latent_size=32):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        # Calculate flattened input size\n",
    "        input_size = input_shape[0] * input_shape[1] * input_shape[2]\n",
    "\n",
    "        self.encoder = VAE_Encoder(input_size=input_size, latent_size=latent_size)\n",
    "        self.decoder = VAE_Decoder(latent_size=latent_size, output_size=input_size, output_shape=input_shape)\n",
    "        self.sampling_layer = Sampling()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mean, z_logvar = self.encoder(x)\n",
    "        kl_loss = KL_Divergence_Loss(z_mean, z_logvar)\n",
    "        z_sample = self.sampling_layer(z_mean, z_logvar)\n",
    "        out = self.decoder(z_sample)\n",
    "        return kl_loss, out\n",
    "\n",
    "\n",
    "def KL_Divergence_Loss(z_mean, z_logvar):\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean ** 2 - torch.exp(z_logvar), dim=1)\n",
    "    kl_loss = kl_loss.mean()\n",
    "    return kl_loss\n",
    "\n",
    "\n",
    "class VAELoss(nn.Module):\n",
    "    def __init__(self, beta=1.0, reduction='sum'):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.mse = nn.MSELoss(reduction=reduction)\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, model_output, target):\n",
    "        kl_loss, reconstruction = model_output\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        recon_loss = self.mse(reconstruction, target)\n",
    "        recon_loss = recon_loss / batch_size\n",
    "\n",
    "        total_loss = recon_loss + self.beta * kl_loss\n",
    "\n",
    "        return total_loss\n"
   ],
   "id": "91db2f5cffee178e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:50.606846Z",
     "start_time": "2025-11-05T21:46:50.573326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_autoencoder(model,\n",
    "                      train_loader,\n",
    "                      val_loader,\n",
    "                      criterion,\n",
    "                      optimizer,\n",
    "                      scheduler=None,\n",
    "                      device=None,\n",
    "                      num_epochs=50,\n",
    "                      threshold=0.1,\n",
    "                      early_stop_patience=5,\n",
    "                      output_dir=\"Models\\\\Output\"):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_pixels = 0\n",
    "        total_pixels = 0\n",
    "\n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(device)\n",
    "            batch_size = x.size(0)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * batch_size\n",
    "\n",
    "            # Calculate accuracy\n",
    "            recon = out[1] if isinstance(out, tuple) else out\n",
    "            pixel_diff = torch.abs(recon - x)\n",
    "            correct_pixels += (pixel_diff < threshold).sum().item()\n",
    "            total_pixels += recon.numel()\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_train_acc = 100.0 * correct_pixels / total_pixels\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracies.append(epoch_train_acc)\n",
    "\n",
    "        # VALIDATION\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct_pixels = 0\n",
    "        val_total_pixels = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, _ in val_loader:\n",
    "                x = x.to(device)\n",
    "                batch_size = x.size(0)\n",
    "                out = model(x)\n",
    "                loss = criterion(out, x)\n",
    "                val_running_loss += loss.item() * batch_size\n",
    "\n",
    "                # Calculate accuracy\n",
    "                recon = out[1] if isinstance(out, tuple) else out\n",
    "                pixel_diff = torch.abs(recon - x)\n",
    "                val_correct_pixels += (pixel_diff < threshold).sum().item()\n",
    "                val_total_pixels += recon.numel()\n",
    "\n",
    "        epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = 100.0 * val_correct_pixels / val_total_pixels\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accuracies.append(epoch_val_acc)\n",
    "\n",
    "        tqdm.write(\n",
    "            f\"Epoch {len(train_losses)}/{num_epochs} | \"\n",
    "            f\"Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f} | \"\n",
    "            f\"Train Acc: {epoch_train_acc:.2f}% | Val Acc: {epoch_val_acc:.2f}%\"\n",
    "        )\n",
    "\n",
    "        # Scheduler step\n",
    "        if scheduler:\n",
    "            scheduler.step(epoch_val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        loss_diff = np.abs(epoch_val_loss - best_val_loss)\n",
    "        if early_stop_patience or loss_diff < threshold:\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= early_stop_patience:\n",
    "                    tqdm.write(f\"Early stopping at epoch {len(train_losses)}\")\n",
    "                    if best_state:\n",
    "                        model.load_state_dict(best_state)\n",
    "                    break\n",
    "\n",
    "    metrics = {\n",
    "        \"train_loss\": train_losses,\n",
    "        \"val_loss\": val_losses,\n",
    "        \"train_accuracy\": train_accuracies,\n",
    "        \"val_accuracy\": val_accuracies,\n",
    "    }\n",
    "\n",
    "    model_path = os.path.join(output_dir, f\"{model.__class__.__name__}_ep{len(train_losses)}.pth\")\n",
    "    metrics_path = os.path.join(output_dir, f\"{model.__class__.__name__}_ep{len(train_losses)}_metrics.pth\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    torch.save(metrics, metrics_path)\n",
    "\n",
    "    tqdm.write(f\"Model saved to: {model_path}\")\n",
    "\n",
    "    return metrics"
   ],
   "id": "b33e7d8b0c7d6e4e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:50.651988Z",
     "start_time": "2025-11-05T21:46:50.630873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_metrics(metrics):\n",
    "    epochs = range(1, len(metrics[\"train_loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, metrics[\"train_loss\"], label=\"train\")\n",
    "    plt.plot(epochs, metrics[\"val_loss\"], label=\"val\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, metrics[\"train_accuracy\"], label=\"train\")\n",
    "    plt.plot(epochs, metrics[\"val_accuracy\"], label=\"val\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Reconstruction Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "39eb3637a9fcea73",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:50.769702Z",
     "start_time": "2025-11-05T21:46:50.709028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_autoencoder(model_base_dir,\n",
    "                     models,\n",
    "                     test_loader,\n",
    "                     model_names=None,\n",
    "                     show_examples=8,\n",
    "                     device=None,\n",
    "                     threshold=0.1):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = torch.device(device)\n",
    "\n",
    "    if not isinstance(models, list):\n",
    "        models = [models]\n",
    "\n",
    "    processed_models = []\n",
    "    extracted_names = []\n",
    "\n",
    "    for item in models:\n",
    "        if isinstance(item, tuple):\n",
    "            if len(item) == 3:\n",
    "                model, load_path, name = item\n",
    "                extracted_names.append(name)\n",
    "            elif len(item) == 2:\n",
    "                model, load_path = item\n",
    "                extracted_names.append(None)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid tuple format: {item}\")\n",
    "        else:\n",
    "            model, load_path = item, None\n",
    "            extracted_names.append(None)\n",
    "\n",
    "        # only join if load_path is not None\n",
    "        load_path = os.path.join(model_base_dir, load_path) if load_path is not None else None\n",
    "        processed_models.append((model, load_path))\n",
    "\n",
    "    num_models = len(processed_models)\n",
    "\n",
    "    if model_names is None:\n",
    "        model_names = []\n",
    "        for i, name in enumerate(extracted_names):\n",
    "            if name is not None:\n",
    "                model_names.append(name)\n",
    "            else:\n",
    "                model_names.append(f\"Model {i + 1}\")\n",
    "\n",
    "    if len(model_names) < num_models:\n",
    "        model_names.extend([f\"Model {i + 1}\" for i in range(len(model_names), num_models)])\n",
    "\n",
    "    all_metrics = {}\n",
    "    all_outputs = []\n",
    "    test_images = None\n",
    "\n",
    "    def to_display_array(tensor):\n",
    "        # tensor: (C, H, W) or (H, W) already\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            arr = tensor.detach().cpu().numpy()\n",
    "        else:\n",
    "            arr = np.array(tensor)\n",
    "        if arr.ndim == 3:\n",
    "            c, h, w = arr.shape\n",
    "            if c == 1:\n",
    "                return arr.squeeze(0)\n",
    "            else:\n",
    "                return np.transpose(arr, (1, 2, 0))  # H, W, C\n",
    "        elif arr.ndim == 2:\n",
    "            return arr\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported image array shape: {arr.shape}\")\n",
    "\n",
    "    for idx, (model, load_path) in enumerate(processed_models):\n",
    "        model.to(device)\n",
    "\n",
    "        if load_path is not None:\n",
    "            state = torch.load(load_path, map_location=device)\n",
    "            model.load_state_dict(state)\n",
    "\n",
    "        model.eval()\n",
    "        correct_pixels = 0\n",
    "        total_pixels = 0\n",
    "        first_batch_outputs = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, _ in test_loader:\n",
    "                x = x.to(device)\n",
    "                out = model(x)\n",
    "\n",
    "                recon = out[1] if isinstance(out, tuple) else out\n",
    "\n",
    "                pixel_diff = torch.abs(recon - x)\n",
    "                correct_pixels += (pixel_diff < threshold).sum().item()\n",
    "                total_pixels += recon.numel()\n",
    "\n",
    "                if show_examples and first_batch_outputs is None:\n",
    "                    first_batch_outputs = recon.detach().cpu()\n",
    "                    if test_images is None:\n",
    "                        test_images = x.detach().cpu()\n",
    "\n",
    "        accuracy = 100.0 * correct_pixels / total_pixels if total_pixels > 0 else 0.0\n",
    "\n",
    "        all_metrics[model_names[idx]] = {\n",
    "            \"test_accuracy\": accuracy\n",
    "        }\n",
    "        all_outputs.append(first_batch_outputs)\n",
    "\n",
    "        print(f\"{model_names[idx]} - Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    if show_examples and test_images is not None:\n",
    "        examples = min(show_examples, test_images.size(0))\n",
    "        num_cols = num_models + 1\n",
    "\n",
    "        figsize = (num_cols * 2, examples * 2)\n",
    "\n",
    "        fig, axes = plt.subplots(examples, num_cols, figsize=figsize)\n",
    "\n",
    "        if examples == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "\n",
    "        for row in range(examples):\n",
    "            orig_arr = to_display_array(test_images[row])\n",
    "            if orig_arr.ndim == 2:\n",
    "                axes[row, 0].imshow(orig_arr, cmap=\"gray\")\n",
    "            else:\n",
    "                axes[row, 0].imshow(orig_arr)\n",
    "            axes[row, 0].axis(\"off\")\n",
    "            if row == 0:\n",
    "                axes[row, 0].set_title(\"Original\", fontsize=10, fontweight='bold')\n",
    "\n",
    "            for col in range(num_models):\n",
    "                recon_batch = all_outputs[col]\n",
    "                if recon_batch is None:\n",
    "                    # blank if no recon available\n",
    "                    axes[row, col + 1].axis(\"off\")\n",
    "                    continue\n",
    "                recon_arr = to_display_array(recon_batch[row])\n",
    "                if recon_arr.ndim == 2:\n",
    "                    axes[row, col + 1].imshow(recon_arr, cmap=\"gray\")\n",
    "                else:\n",
    "                    axes[row, col + 1].imshow(recon_arr)\n",
    "                axes[row, col + 1].axis(\"off\")\n",
    "                if row == 0:\n",
    "                    axes[row, col + 1].set_title(model_names[col], fontsize=10, fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return all_metrics"
   ],
   "id": "8a008519f880db0b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:50.968477Z",
     "start_time": "2025-11-05T21:46:50.817736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MNIST (grayscale 28x28 images)\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "dataset = datasets.MNIST(\n",
    "    root=\"./Datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset_MNIST = datasets.MNIST(\n",
    "    root=\"./Datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ],
   "id": "a36854a582f208ed",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:51.031146Z",
     "start_time": "2025-11-05T21:46:50.994504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # FashionMNIST (grayscale 28x28 images)\n",
    "# transform = transforms.ToTensor()\n",
    "#\n",
    "# dataset = datasets.FashionMNIST(\n",
    "#     root=\"./Datasets\",\n",
    "#     train=True,\n",
    "#     download=True,\n",
    "#     transform=transform\n",
    "# )\n",
    "\n",
    "test_dataset_FashionMNIST = datasets.FashionMNIST(\n",
    "    root=\"./Datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ],
   "id": "cf79a643be3d73cb",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:53.056245Z",
     "start_time": "2025-11-05T21:46:51.055175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CIFAR-10 (RGB 32x32 images)\n",
    "# transform = transforms.ToTensor()\n",
    "#\n",
    "# dataset = datasets.CIFAR10(\n",
    "#     root=\"./Datasets\",\n",
    "#     train=True,\n",
    "#     download=True,\n",
    "#     transform=transform\n",
    "# )\n",
    "\n",
    "test_dataset_CIFAR10 = datasets.CIFAR10(\n",
    "    root=\"./Datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ],
   "id": "ae906a38c6a8799d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:53.400853Z",
     "start_time": "2025-11-05T21:46:53.325452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training variables\n",
    "\n",
    "batch_size = 256\n",
    "num_epochs = 50\n",
    "train_len = int(len(dataset) * 0.9)\n",
    "val_len = len(dataset) - train_len\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_len, val_len])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "8cde80a7a3a883cf",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:53.610025Z",
     "start_time": "2025-11-05T21:46:53.600260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Training the Basic Autoencoder\n",
    "#\n",
    "# model = Basic_Autoencoder(input_shape=(3, 32, 32), latent_size=64)\n",
    "# print(\"Using device:\", device)\n",
    "# model.to(device)\n",
    "#\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer,\n",
    "#     mode='min',\n",
    "#     patience=3,\n",
    "#     factor=0.5\n",
    "# )\n",
    "#\n",
    "# metrics = train_autoencoder(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     criterion,\n",
    "#     optimizer,\n",
    "#     scheduler=scheduler,\n",
    "#     device=device,\n",
    "#     num_epochs=num_epochs,\n",
    "#     early_stop_patience=5,\n",
    "#     output_dir=\"Models\\\\CIFAR10\"\n",
    "# )\n",
    "#\n",
    "# plot_metrics(metrics)"
   ],
   "id": "744db7ccc0eefb94",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:53.688674Z",
     "start_time": "2025-11-05T21:46:53.675049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Training the Deep Autoencoder\n",
    "#\n",
    "# model = Deep_Autoencoder(input_shape=(3, 32, 32), latent_size=64)\n",
    "# print(\"Using device:\", device)\n",
    "# model.to(device)\n",
    "#\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer,\n",
    "#     mode='min',\n",
    "#     patience=3,\n",
    "#     factor=0.5\n",
    "# )\n",
    "#\n",
    "# metrics = train_autoencoder(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     criterion,\n",
    "#     optimizer,\n",
    "#     scheduler=scheduler,\n",
    "#     device=device,\n",
    "#     num_epochs=num_epochs,\n",
    "#     early_stop_patience=5,\n",
    "#     output_dir=\"Models\\\\CIFAR10\"\n",
    "# )\n",
    "#\n",
    "# plot_metrics(metrics)"
   ],
   "id": "cba2aa1b80ed9e9a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:53.760710Z",
     "start_time": "2025-11-05T21:46:53.750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Training the VAE\n",
    "#\n",
    "# model = BetaVAE(input_shape=(3, 32, 32), latent_size=64)\n",
    "# model.to(device)\n",
    "# criterion = VAELoss(beta=1.0)\n",
    "# vae_optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     vae_optimizer,\n",
    "#     mode='min',\n",
    "#     patience=3,\n",
    "#     factor=0.5,\n",
    "# )\n",
    "#\n",
    "# vae_metrics = train_autoencoder(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     criterion,\n",
    "#     vae_optimizer,\n",
    "#     scheduler=scheduler,\n",
    "#     device=device,\n",
    "#     num_epochs=num_epochs,\n",
    "#     early_stop_patience=5,\n",
    "#     output_dir=\"Models\\\\CIFAR10\"\n",
    "# )\n",
    "#\n",
    "# plot_metrics(vae_metrics)\n"
   ],
   "id": "bbe62dee10558303",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:53.869200Z",
     "start_time": "2025-11-05T21:46:53.858007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Testing the autoencoder\n",
    "#\n",
    "# test_loader = DataLoader(test_dataset_CIFAR10, batch_size=batch_size, shuffle=False)\n",
    "#\n",
    "# basic_model = Basic_Autoencoder(input_shape=(3, 32, 32), latent_size=64)\n",
    "# deep_model = Deep_Autoencoder(input_shape=(3, 32, 32), latent_size=64)\n",
    "# vae_model = BetaVAE(input_shape=(3, 32, 32), latent_size=64)\n",
    "#\n",
    "# # model_base_dir = \"Models\\\\MNIST\"\n",
    "# # model_base_dir = \"Models\\\\FashionMNIST\"\n",
    "# model_base_dir = \"Models\\\\CIFAR10\"\n",
    "#\n",
    "# models_to_test = [\n",
    "#     (basic_model, \"Basic_Autoencoder_ep50.pth\", \"Basic AE\"),\n",
    "#     (deep_model, \"Deep_Autoencoder_ep50.pth\", \"Deep AE\"),\n",
    "#     (vae_model, \"BetaVAE_ep50.pth\", \"β-VAE\"),\n",
    "# ]\n",
    "#\n",
    "# test_metrics = test_autoencoder(\n",
    "#     model_base_dir,\n",
    "#     models_to_test,\n",
    "#     test_loader,\n",
    "#     show_examples=8,\n",
    "#     device=device\n",
    "# )\n"
   ],
   "id": "6a019027e158a72a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:53.954395Z",
     "start_time": "2025-11-05T21:46:53.933262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_all_metrics(model_base_dir):\n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(model_base_dir):\n",
    "        print(f\"Directory not found: {model_base_dir}\")\n",
    "        return\n",
    "\n",
    "    # Get all files and filter for metric files\n",
    "    all_files = os.listdir(model_base_dir)\n",
    "    metric_files = [f for f in all_files if f.endswith(\"_metrics.pth\")]\n",
    "\n",
    "    if not metric_files:\n",
    "        print(f\"No metric files found in {model_base_dir}\")\n",
    "        return\n",
    "\n",
    "    # print(f\"Found {len(metric_files)} metric file(s):\\n\")\n",
    "    print(f\"Plotting metrics from directory: {model_base_dir}\\n\")\n",
    "\n",
    "    for filename in sorted(metric_files):\n",
    "        metric_path = os.path.join(model_base_dir, filename)\n",
    "        model_name = filename.replace(\"_metrics.pth\", \"\")\n",
    "\n",
    "        print(f\"Plotting metrics for: {model_name}\")\n",
    "\n",
    "        # Load metrics\n",
    "        metrics = torch.load(metric_path, map_location='cpu')\n",
    "\n",
    "        # Plot using existing function\n",
    "        plot_metrics(metrics)\n",
    "\n",
    "        print(\"-\" * 50)"
   ],
   "id": "452dd31bdf31012e",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:54.027790Z",
     "start_time": "2025-11-05T21:46:54.018429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# plot_all_metrics(\"Models\\\\MNIST\")\n",
    "# plot_all_metrics(\"Models\\\\FashionMNIST\")\n",
    "# plot_all_metrics(\"Models\\\\CIFAR10\")"
   ],
   "id": "f75045d0ba5f0bd2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:54.116463Z",
     "start_time": "2025-11-05T21:46:54.102818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Testing the autoencoders\n",
    "#\n",
    "# test_metrics = test_autoencoder(\n",
    "#     \"Models\\\\MNIST\",\n",
    "#     [(Basic_Autoencoder(),\n",
    "#       \"Basic_Autoencoder_ep50.pth\", \"Basic AE\"),\n",
    "#      (Deep_Autoencoder(),\n",
    "#       \"Deep_Autoencoder_ep50.pth\", \"Deep AE\"),\n",
    "#      (BetaVAE(),\n",
    "#       \"BetaVAE_ep50.pth\", \"β-VAE\"), ],\n",
    "#     DataLoader(test_dataset_MNIST, batch_size=batch_size, shuffle=False),\n",
    "#     show_examples=5,\n",
    "#     device=device\n",
    "# )\n",
    "#\n",
    "# test_metrics = test_autoencoder(\n",
    "#     \"Models\\\\FashionMNIST\",\n",
    "#     [(Basic_Autoencoder(),\n",
    "#       \"Basic_Autoencoder_ep50.pth\", \"Basic AE\"),\n",
    "#      (Deep_Autoencoder(),\n",
    "#       \"Deep_Autoencoder_ep50.pth\", \"Deep AE\"),\n",
    "#      (BetaVAE(),\n",
    "#       \"BetaVAE_ep50.pth\", \"β-VAE\"), ],\n",
    "#     DataLoader(test_dataset_FashionMNIST, batch_size=batch_size, shuffle=False),\n",
    "#     show_examples=5,\n",
    "#     device=device\n",
    "# )\n",
    "#\n",
    "# test_metrics = test_autoencoder(\n",
    "#     \"Models\\\\CIFAR10\",\n",
    "#     [(Basic_Autoencoder(input_shape=(3, 32, 32), latent_size=64),\n",
    "#       \"Basic_Autoencoder_ep50.pth\", \"Basic AE\"),\n",
    "#      (Deep_Autoencoder(input_shape=(3, 32, 32), latent_size=64),\n",
    "#       \"Deep_Autoencoder_ep50.pth\", \"Deep AE\"),\n",
    "#      (BetaVAE(input_shape=(3, 32, 32), latent_size=64),\n",
    "#       \"BetaVAE_ep50.pth\", \"β-VAE\"), ],\n",
    "#     DataLoader(test_dataset_CIFAR10, batch_size=batch_size, shuffle=False),\n",
    "#     show_examples=5,\n",
    "#     device=device\n",
    "# )\n"
   ],
   "id": "3568e6ff7ac6690f",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:54.219804Z",
     "start_time": "2025-11-05T21:46:54.179560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_multiple_beta_vae(\n",
    "        datasets_config,\n",
    "        beta_values=[0.0, 0.5, 1.0, 2.0, 5.0],\n",
    "        batch_size=256,\n",
    "        num_epochs=50,\n",
    "        device=None,\n",
    "        root_dir=\"Models\\\\BetaVAE\"\n",
    "):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Training β-VAE with beta values: {beta_values}\")\n",
    "    print(f\"Datasets: {[cfg['name'] for cfg in datasets_config]}\\n\")\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    for dataset_cfg in datasets_config:\n",
    "        dataset = dataset_cfg['dataset']\n",
    "        dataset_name = dataset_cfg['name']\n",
    "        input_shape = dataset_cfg['input_shape']\n",
    "        latent_size = dataset_cfg['latent_size']\n",
    "\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Dataset: {dataset_name} (latent_size={latent_size})\")\n",
    "        print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "        # Split dataset\n",
    "        train_len = int(len(dataset) * 0.9)\n",
    "        val_len = len(dataset) - train_len\n",
    "        train_dataset, val_dataset = random_split(dataset, [train_len, val_len])\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        dataset_results = {}\n",
    "\n",
    "        for beta in beta_values:\n",
    "            config_name = f\"beta_{beta}\"\n",
    "\n",
    "            print(f\"\\n{'-' * 50}\")\n",
    "            print(f\"Training β-VAE with β={beta} on {dataset_name}\")\n",
    "            print(f\"{'-' * 50}\\n\")\n",
    "\n",
    "            # Create model\n",
    "            model = BetaVAE(input_shape=input_shape, latent_size=latent_size)\n",
    "            model.to(device)\n",
    "\n",
    "            # Create criterion with current beta value\n",
    "            criterion = VAELoss(beta=beta)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='min',\n",
    "                patience=3,\n",
    "                factor=0.5\n",
    "            )\n",
    "\n",
    "            # Create output directory: Models/BetaVAE/[Dataset]/[config]\n",
    "            output_dir = os.path.join(root_dir, dataset_name, config_name)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Train model\n",
    "            metrics = train_autoencoder(\n",
    "                model,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                criterion,\n",
    "                optimizer,\n",
    "                scheduler=scheduler,\n",
    "                device=device,\n",
    "                num_epochs=num_epochs,\n",
    "                early_stop_patience=5,\n",
    "                output_dir=output_dir\n",
    "            )\n",
    "\n",
    "            # Plot metrics\n",
    "            print(f\"\\nPlotting metrics for β={beta}\")\n",
    "            plot_metrics(metrics)\n",
    "\n",
    "            # Store results\n",
    "            dataset_results[config_name] = {\n",
    "                \"metrics\": metrics,\n",
    "                \"model_path\": os.path.join(output_dir, f\"BetaVAE_ep{len(metrics['train_loss'])}.pth\"),\n",
    "                \"beta\": beta,\n",
    "                \"latent_size\": latent_size,\n",
    "                \"final_train_loss\": metrics['train_loss'][-1],\n",
    "                \"final_val_loss\": metrics['val_loss'][-1],\n",
    "                \"final_train_acc\": metrics['train_accuracy'][-1],\n",
    "                \"final_val_acc\": metrics['val_accuracy'][-1]\n",
    "            }\n",
    "\n",
    "        all_results[dataset_name] = dataset_results\n",
    "\n",
    "        # Print summary for this dataset\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Summary for {dataset_name}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        for config_name, results in dataset_results.items():\n",
    "            print(f\"\\n{config_name}:\")\n",
    "            print(f\"  Final Val Loss: {results['final_val_loss']:.4f}\")\n",
    "            print(f\"  Final Val Acc: {results['final_val_acc']:.2f}%\")\n",
    "\n",
    "    return all_results"
   ],
   "id": "b341d02f5517187a",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T21:46:54.270281Z",
     "start_time": "2025-11-05T21:46:54.243884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_all_results_from_metrics(root_dir=\"Models\\\\BetaVAE\"):\n",
    "    if not os.path.exists(root_dir):\n",
    "        print(f\"Directory not found: {root_dir}\")\n",
    "        return {}\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    # Iterate through dataset directories\n",
    "    for dataset_name in os.listdir(root_dir):\n",
    "        dataset_path = os.path.join(root_dir, dataset_name)\n",
    "        if not os.path.isdir(dataset_path):\n",
    "            continue\n",
    "\n",
    "        dataset_results = {}\n",
    "\n",
    "        # Iterate through config directories (beta_X_latent_Y or beta_X)\n",
    "        for config_name in os.listdir(dataset_path):\n",
    "            config_path = os.path.join(dataset_path, config_name)\n",
    "            if not os.path.isdir(config_path):\n",
    "                continue\n",
    "\n",
    "            # Find metrics file\n",
    "            metric_files = [f for f in os.listdir(config_path) if f.endswith(\"_metrics.pth\")]\n",
    "            if not metric_files:\n",
    "                continue\n",
    "\n",
    "            # Load the first (should be only one) metrics file\n",
    "            metrics_file = metric_files[0]\n",
    "            metrics_path = os.path.join(config_path, metrics_file)\n",
    "            metrics = torch.load(metrics_path, map_location='cpu')\n",
    "\n",
    "            # Find model file\n",
    "            model_files = [f for f in os.listdir(config_path) if f.endswith(\".pth\") and not f.endswith(\"_metrics.pth\")]\n",
    "            model_path = os.path.join(config_path, model_files[0]) if model_files else None\n",
    "\n",
    "            # Extract beta and latent_size from config_name\n",
    "            parts = config_name.split(\"_\")\n",
    "            beta = float(parts[1]) if len(parts) > 1 else None\n",
    "            latent_size = int(parts[3]) if len(parts) > 3 else None\n",
    "\n",
    "            dataset_results[config_name] = {\n",
    "                \"metrics\": metrics,\n",
    "                \"model_path\": model_path,\n",
    "                \"beta\": beta,\n",
    "                \"latent_size\": latent_size,\n",
    "                \"final_train_loss\": metrics['train_loss'][-1],\n",
    "                \"final_val_loss\": metrics['val_loss'][-1],\n",
    "                \"final_train_acc\": metrics['train_accuracy'][-1],\n",
    "                \"final_val_acc\": metrics['val_accuracy'][-1]\n",
    "            }\n",
    "\n",
    "        if dataset_results:\n",
    "            all_results[dataset_name] = dataset_results\n",
    "\n",
    "    return all_results\n"
   ],
   "id": "73c87f76abb583d6",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-05T21:46:54.293366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare datasets configuration with latent sizes\n",
    "datasets_config = [\n",
    "    {\n",
    "        'dataset': datasets.MNIST(root=\"./Datasets\", train=True, download=True, transform=transforms.ToTensor()),\n",
    "        'name': 'MNIST',\n",
    "        'input_shape': (1, 28, 28),\n",
    "        'latent_size': 32\n",
    "    },\n",
    "    {\n",
    "        'dataset': datasets.FashionMNIST(root=\"./Datasets\", train=True, download=True, transform=transforms.ToTensor()),\n",
    "        'name': 'FashionMNIST',\n",
    "        'input_shape': (1, 28, 28),\n",
    "        'latent_size': 32\n",
    "    },\n",
    "    {\n",
    "        'dataset': datasets.CIFAR10(root=\"./Datasets\", train=True, download=True, transform=transforms.ToTensor()),\n",
    "        'name': 'CIFAR10',\n",
    "        'input_shape': (3, 32, 32),\n",
    "        'latent_size': 64\n",
    "    }\n",
    "]\n",
    "\n",
    "# Train with different beta values\n",
    "beta_values = [0.0, 0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "all_results = train_multiple_beta_vae(\n",
    "    datasets_config=datasets_config,\n",
    "    beta_values=beta_values,\n",
    "    batch_size=256,\n",
    "    num_epochs=50,\n",
    "    device=device,\n",
    "    root_dir=\"Models\\\\BetaVAE\",\n",
    ")"
   ],
   "id": "7bb6ffaaed76e2f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training β-VAE with beta values: [0.0, 0.5, 1.0, 2.0, 5.0]\n",
      "Datasets: ['MNIST', 'FashionMNIST', 'CIFAR10']\n",
      "\n",
      "\n",
      "============================================================\n",
      "Dataset: MNIST (latent_size=32)\n",
      "============================================================\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training β-VAE with β=0.0 on MNIST\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28172168afba4854a12e24630859b64c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 36.7742 | Val Loss: 17.7553 | Train Acc: 72.48% | Val Acc: 82.58%\n",
      "Epoch 2/50 | Train Loss: 13.6254 | Val Loss: 11.2778 | Train Acc: 85.48% | Val Acc: 87.39%\n",
      "Epoch 3/50 | Train Loss: 9.8622 | Val Loss: 8.9267 | Train Acc: 88.41% | Val Acc: 89.19%\n",
      "Epoch 4/50 | Train Loss: 8.2110 | Val Loss: 7.8339 | Train Acc: 89.72% | Val Acc: 90.07%\n",
      "Epoch 5/50 | Train Loss: 7.2185 | Val Loss: 6.9170 | Train Acc: 90.39% | Val Acc: 90.62%\n",
      "Epoch 6/50 | Train Loss: 6.5400 | Val Loss: 6.4188 | Train Acc: 90.83% | Val Acc: 90.92%\n",
      "Epoch 7/50 | Train Loss: 6.0485 | Val Loss: 6.1983 | Train Acc: 91.15% | Val Acc: 91.05%\n",
      "Epoch 8/50 | Train Loss: 5.6987 | Val Loss: 5.7308 | Train Acc: 91.37% | Val Acc: 91.37%\n",
      "Epoch 9/50 | Train Loss: 5.4112 | Val Loss: 5.4886 | Train Acc: 91.57% | Val Acc: 91.59%\n",
      "Epoch 10/50 | Train Loss: 5.1515 | Val Loss: 5.2374 | Train Acc: 91.74% | Val Acc: 91.72%\n",
      "Epoch 11/50 | Train Loss: 4.9218 | Val Loss: 5.0497 | Train Acc: 91.89% | Val Acc: 91.84%\n",
      "Epoch 12/50 | Train Loss: 4.7325 | Val Loss: 4.8723 | Train Acc: 92.01% | Val Acc: 91.97%\n",
      "Epoch 13/50 | Train Loss: 4.5781 | Val Loss: 4.7460 | Train Acc: 92.12% | Val Acc: 92.03%\n",
      "Epoch 14/50 | Train Loss: 4.4351 | Val Loss: 4.6319 | Train Acc: 92.22% | Val Acc: 92.11%\n",
      "Epoch 15/50 | Train Loss: 4.3177 | Val Loss: 4.4991 | Train Acc: 92.29% | Val Acc: 92.19%\n",
      "Epoch 16/50 | Train Loss: 4.2068 | Val Loss: 4.5100 | Train Acc: 92.37% | Val Acc: 92.19%\n",
      "Epoch 17/50 | Train Loss: 4.1217 | Val Loss: 4.4121 | Train Acc: 92.43% | Val Acc: 92.31%\n",
      "Epoch 18/50 | Train Loss: 4.0400 | Val Loss: 4.3088 | Train Acc: 92.50% | Val Acc: 92.36%\n",
      "Epoch 19/50 | Train Loss: 3.9570 | Val Loss: 4.2304 | Train Acc: 92.56% | Val Acc: 92.44%\n",
      "Epoch 20/50 | Train Loss: 3.8885 | Val Loss: 4.1786 | Train Acc: 92.61% | Val Acc: 92.49%\n",
      "Epoch 21/50 | Train Loss: 3.8308 | Val Loss: 4.1286 | Train Acc: 92.65% | Val Acc: 92.53%\n",
      "Epoch 22/50 | Train Loss: 3.7745 | Val Loss: 4.0884 | Train Acc: 92.70% | Val Acc: 92.53%\n",
      "Epoch 23/50 | Train Loss: 3.7232 | Val Loss: 4.0605 | Train Acc: 92.74% | Val Acc: 92.57%\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# loaded_results = load_all_results_from_metrics(root_dir=\"Models\\\\BetaVAE\")\n",
   "id": "54c043a79412dbd3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "def test_all_models(test_datasets, batch_size, device, models_root=\"Models\\\\BetaVAE\"):\n",
    "    test_results = {}\n",
    "\n",
    "    # Define model configurations for each dataset\n",
    "    dataset_configs = {\n",
    "        'MNIST': {'input_shape': (1, 28, 28), 'latent_size': 32},\n",
    "        'FashionMNIST': {'input_shape': (1, 28, 28), 'latent_size': 32},\n",
    "        'CIFAR10': {'input_shape': (3, 32, 32), 'latent_size': 64}\n",
    "    }\n",
    "\n",
    "    for dataset_name, test_dataset in test_datasets.items():\n",
    "        if dataset_name not in dataset_configs:\n",
    "            print(f\"Skipping {dataset_name}: no configuration found\")\n",
    "            continue\n",
    "\n",
    "        config = dataset_configs[dataset_name]\n",
    "        model_dir = os.path.join(models_root, dataset_name)\n",
    "\n",
    "        if not os.path.exists(model_dir):\n",
    "            print(f\"Skipping {dataset_name}: directory not found\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nTesting all β-VAE models for {dataset_name}\")\n",
    "\n",
    "        # Collect all BetaVAE model files\n",
    "        model_list = []\n",
    "        model_files = [f for f in os.listdir(model_dir) if f.endswith('.pth') and 'BetaVAE' in f]\n",
    "\n",
    "        for model_file in sorted(model_files):\n",
    "            # Extract beta value from filename\n",
    "            beta_match = re.search(r'beta([\\d.]+)', model_file)\n",
    "            if not beta_match:\n",
    "                continue\n",
    "\n",
    "            beta = float(beta_match.group(1))\n",
    "\n",
    "            # Create model instance (beta parameter is only for loss, not model architecture)\n",
    "            model = BetaVAE(**config)\n",
    "            display_name = f\"β-VAE (β={beta})\"\n",
    "            model_list.append((model, model_file, display_name))\n",
    "\n",
    "        if not model_list:\n",
    "            print(f\"No β-VAE models found for {dataset_name}\")\n",
    "            continue\n",
    "\n",
    "        # Test all models for this dataset\n",
    "        results = test_autoencoder(\n",
    "            model_dir,\n",
    "            model_list,\n",
    "            DataLoader(test_dataset, batch_size=batch_size, shuffle=False),\n",
    "            show_examples=5,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        test_results[dataset_name] = results\n",
    "\n",
    "    return test_results"
   ],
   "id": "65598a45f13b7a69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define your test datasets\n",
    "test_datasets = {\n",
    "    'MNIST': test_dataset_MNIST,\n",
    "    'FashionMNIST': test_dataset_FashionMNIST,\n",
    "    'CIFAR10': test_dataset_CIFAR10\n",
    "}\n",
    "\n",
    "# Run the test function\n",
    "all_test_results = test_all_models(\n",
    "    test_datasets=test_datasets,\n",
    "    batch_size=32,\n",
    "    device=device,\n",
    "    models_root=\"Models\"\n",
    ")"
   ],
   "id": "b2a56aef95dadfd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
